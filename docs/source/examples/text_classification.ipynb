{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Calibrating Sentiment Analysis Across Domains\n",
    "\n",
    "This example demonstrates rank-preserving calibration for text classification, specifically sentiment analysis. We'll show how models trained on one domain (e.g., movie reviews) can be calibrated for deployment in different domains with different sentiment distributions.\n",
    "\n",
    "## Business Motivation\n",
    "\n",
    "Sentiment analysis models face several challenges when deployed across domains:\n",
    "- **Domain shift**: Movie reviews vs. product reviews vs. social media posts\n",
    "- **Sentiment distribution shift**: Different platforms have different baseline sentiment\n",
    "- **Calibration issues**: Models are often overconfident in their predictions\n",
    "\n",
    "Rank-preserving calibration helps maintain the relative ordering of texts by sentiment intensity while adjusting absolute probabilities to match the target domain's sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our calibration package - proper imports\n",
    "from rank_preserving_calibration import calibrate_dykstra\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(['#3498db', '#e74c3c', '#f39c12'])  # Blue, Red, Orange\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸ“° TEXT CLASSIFICATION CALIBRATION TOOLKIT LOADED\")\n",
    "print(\"Focus: Cross-domain sentiment analysis with rank preservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Simulating Multi-Domain Sentiment\n",
    "\n",
    "We'll use the 20 Newsgroups dataset and create a sentiment classification task by treating different newsgroup categories as having different sentiment characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 20 newsgroups data for text classification\n",
    "# We'll select categories that naturally have different \"sentiment\" characteristics\n",
    "categories = [\n",
    "    'alt.atheism',  # Often critical/negative discussions\n",
    "    'rec.sport.baseball',  # Generally positive/enthusiastic\n",
    "    'sci.space',  # Neutral/informative\n",
    "    'talk.politics.misc',  # Mixed sentiment, often contentious\n",
    "    'comp.graphics',  # Technical/neutral\n",
    "    'rec.motorcycles'  # Enthusiastic/positive\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=('headers', 'footers', 'quotes')  # Remove metadata\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(newsgroups.data)} documents from {len(categories)} categories\")\n",
    "print(f\"Categories: {newsgroups.target_names}\")\n",
    "print(f\"Category distribution: {Counter(newsgroups.target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sentiment Labels and Domain Structure\n",
    "\n",
    "We'll create a realistic sentiment classification scenario by:\n",
    "1. Simulating sentiment labels based on category characteristics\n",
    "2. Creating training and target domains with different sentiment distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment mapping based on category characteristics\n",
    "# This simulates how different domains have different baseline sentiment\n",
    "category_sentiment_bias = {\n",
    "    'alt.atheism': {'negative': 0.4, 'neutral': 0.4, 'positive': 0.2},\n",
    "    'rec.sport.baseball': {'negative': 0.1, 'neutral': 0.3, 'positive': 0.6},\n",
    "    'sci.space': {'negative': 0.1, 'neutral': 0.7, 'positive': 0.2},\n",
    "    'talk.politics.misc': {'negative': 0.5, 'neutral': 0.3, 'positive': 0.2},\n",
    "    'comp.graphics': {'negative': 0.2, 'neutral': 0.6, 'positive': 0.2},\n",
    "    'rec.motorcycles': {'negative': 0.15, 'neutral': 0.25, 'positive': 0.6}\n",
    "}\n",
    "\n",
    "# Function to generate sentiment labels based on category\n",
    "def generate_sentiment_labels(targets, target_names, sentiment_mapping, random_state=42):\n",
    "    \"\"\"Generate sentiment labels based on category bias.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    labels = []\n",
    "\n",
    "    for target in targets:\n",
    "        category = target_names[target]\n",
    "        probs = sentiment_mapping[category]\n",
    "\n",
    "        # Sample sentiment based on category bias\n",
    "        sentiment = np.random.choice(\n",
    "            ['negative', 'neutral', 'positive'],\n",
    "            p=[probs['negative'], probs['neutral'], probs['positive']]\n",
    "        )\n",
    "\n",
    "        # Convert to numeric labels\n",
    "        if sentiment == 'negative':\n",
    "            labels.append(0)\n",
    "        elif sentiment == 'neutral':\n",
    "            labels.append(1)\n",
    "        else:  # positive\n",
    "            labels.append(2)\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "# Generate sentiment labels\n",
    "sentiment_labels = generate_sentiment_labels(\n",
    "    newsgroups.target, newsgroups.target_names, category_sentiment_bias\n",
    ")\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "sentiment_counts = Counter(sentiment_labels)\n",
    "sentiment_names = ['Negative', 'Neutral', 'Positive']\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    count = sentiment_counts[i]\n",
    "    pct = count / len(sentiment_labels) * 100\n",
    "    print(f\"  {name}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show distribution by category\n",
    "print(\"\\nSentiment by Category:\")\n",
    "for i, category in enumerate(newsgroups.target_names):\n",
    "    mask = newsgroups.target == i\n",
    "    cat_sentiments = sentiment_labels[mask]\n",
    "    dist = [np.mean(cat_sentiments == j) for j in range(3)]\n",
    "    print(f\"  {category}: Neg={dist[0]:.2f}, Neu={dist[1]:.2f}, Pos={dist[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Feature Extraction\n",
    "\n",
    "We'll create TF-IDF features for our text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove very short texts (likely corrupted)\n",
    "    if len(text.split()) < 5:\n",
    "        return \"\"\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess all texts\n",
    "processed_texts = [preprocess_text(text) for text in newsgroups.data]\n",
    "\n",
    "# Filter out empty texts\n",
    "valid_indices = [i for i, text in enumerate(processed_texts) if text != \"\"]\n",
    "texts = [processed_texts[i] for i in valid_indices]\n",
    "targets = newsgroups.target[valid_indices]\n",
    "sentiments = sentiment_labels[valid_indices]\n",
    "\n",
    "print(f\"After preprocessing: {len(texts)} valid documents\")\n",
    "print(f\"Average document length: {np.mean([len(text.split()) for text in texts]):.1f} words\")\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Feature matrix sparsity: {(1 - X.nnz / X.size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: Domain-Specific Sentiment Classifier\n",
    "\n",
    "We'll train a sentiment classifier and then simulate deploying it to a domain with different sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test, target_train, target_test = train_test_split(\n",
    "    X, sentiments, targets, test_size=0.3, random_state=42, stratify=sentiments\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} documents\")\n",
    "print(f\"Test set: {X_test.shape[0]} documents\")\n",
    "\n",
    "# Training sentiment distribution\n",
    "train_sentiment_dist = np.bincount(y_train) / len(y_train)\n",
    "print(\"\\nTraining sentiment distribution:\")\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    print(f\"  {name}: {train_sentiment_dist[i]:.3f}\")\n",
    "\n",
    "# Train logistic regression classifier\n",
    "clf = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    multi_class='ovr',  # One-vs-rest for interpretability\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Multi-class AUC (one-vs-rest)\n",
    "auc_scores = []\n",
    "for i in range(3):\n",
    "    y_binary = (y_test == i).astype(int)\n",
    "    auc = roc_auc_score(y_binary, y_proba[:, i])\n",
    "    auc_scores.append(auc)\n",
    "    print(f\"AUC {sentiment_names[i]}: {auc:.3f}\")\n",
    "\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.3f}\")\n",
    "\n",
    "# Show predicted vs actual sentiment distribution\n",
    "test_sentiment_dist = np.bincount(y_test) / len(y_test)\n",
    "pred_sentiment_dist = y_proba.mean(axis=0)\n",
    "\n",
    "print(\"\\nTest set sentiment distribution:\")\n",
    "print(f\"  Actual: {test_sentiment_dist}\")\n",
    "print(f\"  Predicted: {pred_sentiment_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation: Domain Shift Scenario\n",
    "\n",
    "We'll simulate deploying our model to a new domain (e.g., social media) with a different sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate target domain with different sentiment distribution\n",
    "# Example: Social media platform with more positive content\n",
    "target_domain = \"Social Media Platform\"\n",
    "target_sentiment_distribution = np.array([0.20, 0.25, 0.55])  # More positive sentiment\n",
    "\n",
    "print(f\"Domain Shift Scenario: Deploying to {target_domain}\")\n",
    "print(\"\\nSentiment Distribution Comparison:\")\n",
    "print(f\"{'Sentiment':<12} {'Training':<12} {'Target':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    diff = target_sentiment_distribution[i] - train_sentiment_dist[i]\n",
    "    print(f\"{name:<12} {train_sentiment_dist[i]:<12.3f} {target_sentiment_distribution[i]:<12.3f} {diff:<+12.3f}\")\n",
    "\n",
    "# Calculate target marginals for calibration\n",
    "n_samples = len(X_test)\n",
    "target_marginals = target_sentiment_distribution * n_samples\n",
    "\n",
    "print(f\"\\nTarget marginals for {n_samples} samples:\")\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    print(f\"  {name}: {target_marginals[i]:.1f} samples\")\n",
    "\n",
    "print(\"\\nCurrent model predictions:\")\n",
    "current_marginals = y_proba.sum(axis=0)\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    print(f\"  {name}: {current_marginals[i]:.1f} samples\")\n",
    "\n",
    "print(\"\\nMarginal adjustments needed:\")\n",
    "marginal_diffs = target_marginals - current_marginals\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    print(f\"  {name}: {marginal_diffs[i]:+.1f} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Rank-Preserving Calibration\n",
    "\n",
    "Now we'll calibrate the model's predictions to match the target domain's sentiment distribution while preserving the relative ranking of texts by sentiment intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rank-preserving calibration\n",
    "print(\"Applying rank-preserving calibration...\")\n",
    "result = calibrate_dykstra(y_proba, target_marginals, verbose=True, max_iters=3000)\n",
    "\n",
    "print(\"\\nCalibration Results:\")\n",
    "print(f\"Converged: {result.converged}\")\n",
    "print(f\"Iterations: {result.iterations}\")\n",
    "print(f\"Final change: {result.final_change:.2e}\")\n",
    "print(f\"Max row error: {result.max_row_error:.2e}\")\n",
    "print(f\"Max column error: {result.max_col_error:.2e}\")\n",
    "print(f\"Max rank violation: {result.max_rank_violation:.2e}\")\n",
    "\n",
    "# Get calibrated probabilities\n",
    "y_proba_calibrated = result.Q\n",
    "\n",
    "# Verify calibration worked\n",
    "calibrated_marginals = y_proba_calibrated.sum(axis=0)\n",
    "print(\"\\nVerification - Calibrated marginals:\")\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    error = abs(calibrated_marginals[i] - target_marginals[i])\n",
    "    print(f\"  {name}: {calibrated_marginals[i]:.1f} (target: {target_marginals[i]:.1f}, error: {error:.1e})\")\n",
    "\n",
    "# Check rank preservation\n",
    "rank_preservation_check = True\n",
    "for j in range(3):\n",
    "    original_order = np.argsort(y_proba[:, j])\n",
    "    calibrated_order = np.argsort(y_proba_calibrated[:, j])\n",
    "    if not np.array_equal(original_order, calibrated_order):\n",
    "        rank_preservation_check = False\n",
    "        break\n",
    "\n",
    "print(f\"\\nRank preservation: {'âœ“ Perfect' if rank_preservation_check else 'âš  Approximate'}\")\n",
    "\n",
    "# Calculate sentiment distribution changes\n",
    "original_mean_probs = y_proba.mean(axis=0)\n",
    "calibrated_mean_probs = y_proba_calibrated.mean(axis=0)\n",
    "\n",
    "print(\"\\nMean probability changes:\")\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    change = calibrated_mean_probs[i] - original_mean_probs[i]\n",
    "    print(f\"  {name}: {original_mean_probs[i]:.3f} â†’ {calibrated_mean_probs[i]:.3f} ({change:+.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization: Before and After Calibration\n",
    "\n",
    "Let's visualize the impact of calibration on sentiment predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Text Classification: Impact of Rank-Preserving Calibration', fontsize=16, y=0.98)\n",
    "\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71']  # Red, Orange, Green for Negative, Neutral, Positive\n",
    "\n",
    "# 1. Sentiment distribution comparison\n",
    "x_pos = np.arange(3)\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x_pos - width, train_sentiment_dist, width, label='Training', alpha=0.8, color=colors)\n",
    "axes[0, 0].bar(x_pos, original_mean_probs, width, label='Original Model', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width, target_sentiment_distribution, width, label='Target Domain', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Sentiment Class')\n",
    "axes[0, 0].set_ylabel('Proportion')\n",
    "axes[0, 0].set_title('Sentiment Distribution Comparison')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(sentiment_names)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Probability transformation for each class\n",
    "for i, (name, color) in enumerate(zip(sentiment_names, colors, strict=False)):\n",
    "    axes[0, 1].scatter(y_proba[:, i], y_proba_calibrated[:, i], alpha=0.6, s=20, color=color, label=name)\n",
    "\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Identity')\n",
    "axes[0, 1].set_xlabel('Original Probability')\n",
    "axes[0, 1].set_ylabel('Calibrated Probability')\n",
    "axes[0, 1].set_title('Probability Transformation by Class')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribution of probability changes\n",
    "prob_changes = y_proba_calibrated - y_proba\n",
    "for i, (name, color) in enumerate(zip(sentiment_names, colors, strict=False)):\n",
    "    axes[0, 2].hist(prob_changes[:, i], bins=30, alpha=0.7, label=name, color=color, density=True)\n",
    "\n",
    "axes[0, 2].axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0, 2].set_xlabel('Probability Change (Calibrated - Original)')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Distribution of Probability Changes')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confidence distribution changes\n",
    "max_probs_original = np.max(y_proba, axis=1)\n",
    "max_probs_calibrated = np.max(y_proba_calibrated, axis=1)\n",
    "\n",
    "axes[1, 0].hist(max_probs_original, bins=30, alpha=0.7, label='Original', density=True)\n",
    "axes[1, 0].hist(max_probs_calibrated, bins=30, alpha=0.7, label='Calibrated', density=True)\n",
    "axes[1, 0].set_xlabel('Maximum Probability (Confidence)')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Confidence Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Rank preservation visualization\n",
    "# Show correlation between original and calibrated probabilities\n",
    "\n",
    "correlations = []\n",
    "for i, (name, color) in enumerate(zip(sentiment_names, colors, strict=False)):\n",
    "    corr, _ = spearmanr(y_proba[:, i], y_proba_calibrated[:, i])\n",
    "    correlations.append(corr)\n",
    "\n",
    "    # Show top and bottom examples for this class\n",
    "    top_indices = np.argsort(y_proba[:, i])[-10:]\n",
    "    axes[1, 1].scatter(y_proba[top_indices, i], y_proba_calibrated[top_indices, i],\n",
    "                      color=color, alpha=0.8, s=50, label=f'{name} (top 10)')\n",
    "\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Original Probability')\n",
    "axes[1, 1].set_ylabel('Calibrated Probability')\n",
    "axes[1, 1].set_title('Rank Preservation (Top Predictions)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Decision boundary changes\n",
    "thresholds = np.linspace(0.1, 0.9, 9)\n",
    "decision_changes_by_threshold = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    original_decisions = np.argmax(y_proba, axis=1)\n",
    "    calibrated_decisions = np.argmax(y_proba_calibrated, axis=1)\n",
    "\n",
    "    # Count confident predictions above threshold\n",
    "    original_confident = np.max(y_proba, axis=1) > thresh\n",
    "    calibrated_confident = np.max(y_proba_calibrated, axis=1) > thresh\n",
    "\n",
    "    # Count decision changes among confident predictions\n",
    "    confident_mask = original_confident | calibrated_confident\n",
    "    if np.any(confident_mask):\n",
    "        changes = np.sum((original_decisions != calibrated_decisions) & confident_mask)\n",
    "        total_confident = np.sum(confident_mask)\n",
    "        change_rate = changes / total_confident * 100\n",
    "    else:\n",
    "        change_rate = 0\n",
    "\n",
    "    decision_changes_by_threshold.append(change_rate)\n",
    "\n",
    "axes[1, 2].plot(thresholds, decision_changes_by_threshold, 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 2].set_xlabel('Confidence Threshold')\n",
    "axes[1, 2].set_ylabel('Decision Changes (%)')\n",
    "axes[1, 2].set_title('Impact on High-Confidence Decisions')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation summary\n",
    "print(\"\\nRank Preservation Summary (Spearman Correlations):\")\n",
    "for i, (name, corr) in enumerate(zip(sentiment_names, correlations, strict=False)):\n",
    "    print(f\"  {name}: {corr:.6f}\")\n",
    "print(f\"  Mean: {np.mean(correlations):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis: Model Quality Metrics\n",
    "\n",
    "Let's evaluate how calibration affects various performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics comparison\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "\n",
    "def calculate_multiclass_metrics(y_true, y_proba):\n",
    "    \"\"\"Calculate various multiclass performance metrics.\"\"\"\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'log_loss': log_loss(y_true, y_proba),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    # Per-class AUC\n",
    "    auc_scores = []\n",
    "    for i in range(y_proba.shape[1]):\n",
    "        y_binary = (y_true == i).astype(int)\n",
    "        auc = roc_auc_score(y_binary, y_proba[:, i])\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    metrics['auc_macro'] = np.mean(auc_scores)\n",
    "    metrics['auc_per_class'] = auc_scores\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
    "    \"\"\"Calculate Expected Calibration Error for multiclass.\"\"\"\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    confidences = np.max(y_proba, axis=1)\n",
    "    accuracies = (y_pred == y_true).astype(float)\n",
    "\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers, strict=False):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "# Calculate metrics for original and calibrated predictions\n",
    "metrics_original = calculate_multiclass_metrics(y_test, y_proba)\n",
    "metrics_calibrated = calculate_multiclass_metrics(y_test, y_proba_calibrated)\n",
    "\n",
    "ece_original = expected_calibration_error(y_test, y_proba)\n",
    "ece_calibrated = expected_calibration_error(y_test, y_proba_calibrated)\n",
    "\n",
    "# Create comparison table\n",
    "print(\"Performance Metrics Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<20} {'Original':<12} {'Calibrated':<12} {'Change':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric in ['accuracy', 'log_loss', 'f1_macro', 'f1_weighted', 'auc_macro']:\n",
    "    orig_val = metrics_original[metric]\n",
    "    cal_val = metrics_calibrated[metric]\n",
    "    change = cal_val - orig_val\n",
    "    print(f\"{metric:<20} {orig_val:<12.4f} {cal_val:<12.4f} {change:<+12.4f}\")\n",
    "\n",
    "print(f\"{'ECE':<20} {ece_original:<12.4f} {ece_calibrated:<12.4f} {ece_calibrated-ece_original:<+12.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class AUC Comparison:\")\n",
    "print(f\"{'Class':<12} {'Original':<12} {'Calibrated':<12} {'Change':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    orig_auc = metrics_original['auc_per_class'][i]\n",
    "    cal_auc = metrics_calibrated['auc_per_class'][i]\n",
    "    change = cal_auc - orig_auc\n",
    "    print(f\"{name:<12} {orig_auc:<12.4f} {cal_auc:<12.4f} {change:<+12.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Observations:\")\n",
    "print(f\"â€¢ Discrimination preserved: AUC change = {metrics_calibrated['auc_macro'] - metrics_original['auc_macro']:+.6f}\")\n",
    "print(f\"â€¢ Calibration improved: ECE reduced by {ece_original - ece_calibrated:.4f}\")\n",
    "print(f\"â€¢ Accuracy {'improved' if metrics_calibrated['accuracy'] > metrics_original['accuracy'] else 'maintained'}: {metrics_original['accuracy']:.4f} â†’ {metrics_calibrated['accuracy']:.4f}\")\n",
    "print(f\"â€¢ Domain adaptation: Distribution shifted to match target ({target_domain})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Application: Content Moderation Analysis\n",
    "\n",
    "Let's demonstrate how this calibration would work in a practical content moderation scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content moderation scenario analysis\n",
    "def analyze_content_moderation_impact(y_proba_orig, y_proba_cal, sentiment_threshold=0.7):\n",
    "    \"\"\"Analyze impact on content moderation decisions.\"\"\"\n",
    "\n",
    "    # Original moderation decisions\n",
    "    # Flag content with high negative sentiment probability\n",
    "    flag_negative_orig = y_proba_orig[:, 0] > sentiment_threshold  # Negative sentiment > threshold\n",
    "    flag_negative_cal = y_proba_cal[:, 0] > sentiment_threshold\n",
    "\n",
    "    # Promote content with high positive sentiment probability\n",
    "    promote_positive_orig = y_proba_orig[:, 2] > sentiment_threshold  # Positive sentiment > threshold\n",
    "    promote_positive_cal = y_proba_cal[:, 2] > sentiment_threshold\n",
    "\n",
    "    results = {\n",
    "        'flagged_negative': {\n",
    "            'original': np.sum(flag_negative_orig),\n",
    "            'calibrated': np.sum(flag_negative_cal),\n",
    "            'changes': np.sum(flag_negative_orig != flag_negative_cal)\n",
    "        },\n",
    "        'promoted_positive': {\n",
    "            'original': np.sum(promote_positive_orig),\n",
    "            'calibrated': np.sum(promote_positive_cal),\n",
    "            'changes': np.sum(promote_positive_orig != promote_positive_cal)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Analyze at different thresholds\n",
    "thresholds_to_test = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "moderation_results = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    result = analyze_content_moderation_impact(y_proba, y_proba_calibrated, thresh)\n",
    "    result['threshold'] = thresh\n",
    "    moderation_results.append(result)\n",
    "\n",
    "# Display results\n",
    "print(f\"Content Moderation Impact Analysis (Target Domain: {target_domain})\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total content pieces: {len(y_test)}\")\n",
    "print(f\"Target sentiment distribution: Negative={target_sentiment_distribution[0]:.1%}, Neutral={target_sentiment_distribution[1]:.1%}, Positive={target_sentiment_distribution[2]:.1%}\")\n",
    "print()\n",
    "\n",
    "print(\"Negative Content Flagging:\")\n",
    "print(f\"{'Threshold':<10} {'Original':<10} {'Calibrated':<12} {'Changes':<10} {'Change %':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for result in moderation_results:\n",
    "    thresh = result['threshold']\n",
    "    orig = result['flagged_negative']['original']\n",
    "    cal = result['flagged_negative']['calibrated']\n",
    "    changes = result['flagged_negative']['changes']\n",
    "    change_pct = changes / len(y_test) * 100\n",
    "    print(f\"{thresh:<10.1f} {orig:<10} {cal:<12} {changes:<10} {change_pct:<10.1f}%\")\n",
    "\n",
    "print(\"\\nPositive Content Promotion:\")\n",
    "print(f\"{'Threshold':<10} {'Original':<10} {'Calibrated':<12} {'Changes':<10} {'Change %':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for result in moderation_results:\n",
    "    thresh = result['threshold']\n",
    "    orig = result['promoted_positive']['original']\n",
    "    cal = result['promoted_positive']['calibrated']\n",
    "    changes = result['promoted_positive']['changes']\n",
    "    change_pct = changes / len(y_test) * 100\n",
    "    print(f\"{thresh:<10.1f} {orig:<10} {cal:<12} {changes:<10} {change_pct:<10.1f}%\")\n",
    "\n",
    "# Show sample texts affected by calibration\n",
    "thresh_example = 0.7\n",
    "flag_changes = (y_proba[:, 0] > thresh_example) != (y_proba_calibrated[:, 0] > thresh_example)\n",
    "promo_changes = (y_proba[:, 2] > thresh_example) != (y_proba_calibrated[:, 2] > thresh_example)\n",
    "\n",
    "print(f\"\\nðŸ“ Example Content Affected by Calibration (threshold={thresh_example}):\")\n",
    "\n",
    "if np.any(flag_changes):\n",
    "    changed_indices = np.where(flag_changes)[0][:3]  # Show first 3 examples\n",
    "    print(\"\\nNegative Content Flagging Changes:\")\n",
    "    for idx in changed_indices:\n",
    "        text = texts[list(X_test.indices)[idx] if hasattr(X_test, 'indices') else idx][:100]\n",
    "        orig_prob = y_proba[idx, 0]\n",
    "        cal_prob = y_proba_calibrated[idx, 0]\n",
    "        action = \"Flagged\" if cal_prob > thresh_example else \"Unflagged\"\n",
    "        print(f\"   Text: \\\"{text}...\\\"\")\n",
    "        print(f\"   Negative prob: {orig_prob:.3f} â†’ {cal_prob:.3f} ({action})\")\n",
    "        print()\n",
    "\n",
    "if np.any(promo_changes):\n",
    "    changed_indices = np.where(promo_changes)[0][:3]  # Show first 3 examples\n",
    "    print(\"Positive Content Promotion Changes:\")\n",
    "    for idx in changed_indices:\n",
    "        text = texts[list(X_test.indices)[idx] if hasattr(X_test, 'indices') else idx][:100]\n",
    "        orig_prob = y_proba[idx, 2]\n",
    "        cal_prob = y_proba_calibrated[idx, 2]\n",
    "        action = \"Promoted\" if cal_prob > thresh_example else \"Not promoted\"\n",
    "        print(f\"   Text: \\\"{text}...\\\"\")\n",
    "        print(f\"   Positive prob: {orig_prob:.3f} â†’ {cal_prob:.3f} ({action})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact Summary\n",
    "\n",
    "Let's summarize the business value of rank-preserving calibration for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BUSINESS IMPACT SUMMARY: Text Classification Calibration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸŽ¯ SCENARIO:\")\n",
    "print(\"   Model trained on newsgroup data (varied sentiment distribution)\")\n",
    "print(f\"   Deployed to {target_domain} with {target_sentiment_distribution[2]:.0%} positive content\")\n",
    "print(\"   Need to maintain content ranking while adjusting sentiment probabilities\")\n",
    "\n",
    "print(\"\\nðŸ“Š CALIBRATION RESULTS:\")\n",
    "original_dist = y_proba.mean(axis=0)\n",
    "calibrated_dist = y_proba_calibrated.mean(axis=0)\n",
    "print(\"   âœ“ Sentiment distribution adjusted:\")\n",
    "for i, name in enumerate(sentiment_names):\n",
    "    print(f\"     {name}: {original_dist[i]:.3f} â†’ {calibrated_dist[i]:.3f} (target: {target_sentiment_distribution[i]:.3f})\")\n",
    "print(f\"   âœ“ Ranking preserved: Mean Spearman correlation = {np.mean(correlations):.6f}\")\n",
    "print(f\"   âœ“ Discrimination maintained: AUC change = {metrics_calibrated['auc_macro'] - metrics_original['auc_macro']:+.6f}\")\n",
    "\n",
    "print(\"\\nðŸ’¼ BUSINESS BENEFITS:\")\n",
    "print(\"   â€¢ Accurate content prioritization for target audience\")\n",
    "print(\"   â€¢ Consistent user experience across different content sources\")\n",
    "print(\"   â€¢ Improved content moderation efficiency\")\n",
    "print(\"   â€¢ Better alignment with platform-specific engagement patterns\")\n",
    "\n",
    "# Calculate economic impact example\n",
    "baseline_engagement = 0.15  # 15% baseline engagement rate\n",
    "positive_boost = 1.3  # 30% boost from positive content\n",
    "negative_penalty = 0.7  # 30% penalty from negative content\n",
    "\n",
    "# Estimate engagement impact\n",
    "orig_positive_weight = original_dist[2]\n",
    "orig_negative_weight = original_dist[0]\n",
    "cal_positive_weight = calibrated_dist[2]\n",
    "cal_negative_weight = calibrated_dist[0]\n",
    "\n",
    "orig_engagement_factor = (orig_positive_weight * positive_boost +\n",
    "                         orig_negative_weight * negative_penalty +\n",
    "                         original_dist[1] * 1.0)\n",
    "cal_engagement_factor = (cal_positive_weight * positive_boost +\n",
    "                        cal_negative_weight * negative_penalty +\n",
    "                        calibrated_dist[1] * 1.0)\n",
    "\n",
    "engagement_improvement = (cal_engagement_factor / orig_engagement_factor - 1) * 100\n",
    "\n",
    "print(\"\\nðŸ“ˆ ESTIMATED ENGAGEMENT IMPACT:\")\n",
    "print(\"   â€¢ Content sentiment better matches target audience preferences\")\n",
    "print(f\"   â€¢ Estimated engagement improvement: {engagement_improvement:+.1f}%\")\n",
    "print(\"   â€¢ Reduced over-moderation of content in positive-leaning platform\")\n",
    "print(\"   â€¢ Better content recommendation relevance\")\n",
    "\n",
    "print(\"\\nðŸ”§ IMPLEMENTATION CONSIDERATIONS:\")\n",
    "print(\"   â€¢ Requires reliable estimates of target domain sentiment distribution\")\n",
    "print(\"   â€¢ Should be monitored for drift over time\")\n",
    "print(\"   â€¢ Consider A/B testing to validate engagement improvements\")\n",
    "print(\"   â€¢ May need periodic recalibration as content patterns evolve\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ USE CASES FOR TEXT CLASSIFICATION CALIBRATION:\")\n",
    "print(\"   â€¢ Cross-platform content deployment (Twitter â†’ LinkedIn â†’ Reddit)\")\n",
    "print(\"   â€¢ International market adaptation (cultural sentiment differences)\")\n",
    "print(\"   â€¢ Content personalization at scale\")\n",
    "print(\"   â€¢ Multi-brand content strategy alignment\")\n",
    "print(\"   â€¢ Seasonal campaign optimization\")\n",
    "\n",
    "# Show final calibration quality\n",
    "marginal_errors = np.abs(y_proba_calibrated.sum(axis=0) - target_marginals)\n",
    "max_marginal_error = np.max(marginal_errors)\n",
    "print(\"\\nâœ… CALIBRATION QUALITY:\")\n",
    "print(f\"   â€¢ Maximum marginal error: {max_marginal_error:.2e}\")\n",
    "print(f\"   â€¢ Converged in {result.iterations} iterations\")\n",
    "print(\"   â€¢ All constraints satisfied within numerical precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This example demonstrated rank-preserving calibration for text classification across domains. The approach is broadly applicable to:\n",
    "\n",
    "- **Multi-language sentiment analysis** with language-specific sentiment distributions\n",
    "- **Topic classification** across different content sources\n",
    "- **Spam detection** with varying spam rates across platforms\n",
    "- **Content recommendation** systems with audience-specific preferences\n",
    "- **Brand monitoring** across different social media platforms\n",
    "\n",
    "For more real-world examples, see the other notebooks in this series:\n",
    "- Medical diagnosis with population-specific disease prevalence\n",
    "- Image classification with domain adaptation\n",
    "- Financial risk assessment with portfolio-specific distributions\n",
    "- Survey reweighting for demographic representation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}