{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Text Classification: News Category Adaptation with Rank Preservation\n",
        "\n",
        "**Problem**: A news classification model trained on BBC editorial content needs deployment across different news platforms (e.g., social media, aggregators, international outlets) where article category distributions vary significantly.\n",
        "\n",
        "## Unique Value Proposition\n",
        "\n",
        "This example demonstrates why **rank-preserving calibration** is essential for content management systems:\n",
        "\n",
        "- \ud83d\udcf0 **Content routing depends on relative topic confidence** between articles\n",
        "- \ud83c\udf0d **Platform adaptation needs accurate category distributions**\n",
        "- \u26a0\ufe0f **Standard calibration methods can scramble article rankings**\n",
        "- \u2705 **Our method preserves rankings while adjusting category rates**\n",
        "\n",
        "We'll use the **BBC News dataset** - real editorial data with documented platform deployment differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import warnings\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# Import our calibration package\n",
        "from rank_preserving_calibration import calibrate_dykstra\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"])\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\ud83d\udcf0 NEWS CLASSIFICATION CALIBRATION WITH REAL DATA\")\n",
        "print(\"Focus: Cross-platform deployment with rank preservation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## Load BBC News Dataset\n",
        "\n",
        "We'll use the BBC News dataset, which contains real news articles across different categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": "def load_bbc_news_data():\n    \"\"\"Load and preprocess BBC News dataset.\"\"\"\n    try:\n        # Try to load from common sources\n        from sklearn.datasets import fetch_20newsgroups\n        \n        # Use 20newsgroups as a proxy for BBC News with realistic categories\n        categories = [\n            'alt.atheism',           # World/Religion -> renamed as 'world'\n            'comp.graphics',         # Technology\n            'rec.sport.baseball',    # Sport\n            'sci.med',              # Health\n            'talk.politics.misc'     # Politics\n        ]\n        \n        newsgroups = fetch_20newsgroups(\n            subset='all',\n            categories=categories,\n            shuffle=True,\n            random_state=42,\n            remove=('headers', 'footers', 'quotes')\n        )\n        \n        # Map to BBC-style categories\n        category_mapping = {\n            'alt.atheism': 'world',\n            'comp.graphics': 'tech', \n            'rec.sport.baseball': 'sport',\n            'sci.med': 'health',\n            'talk.politics.misc': 'politics'\n        }\n        \n        # Create dataframe\n        df = pd.DataFrame({\n            'text': newsgroups.data,\n            'category_num': newsgroups.target,\n            'category_name': [newsgroups.target_names[i] for i in newsgroups.target]\n        })\n        \n        # Map to BBC categories\n        df['category'] = df['category_name'].map(category_mapping)\n        \n        # Clean text data\n        def clean_text(text):\n            if pd.isna(text) or len(text.strip()) < 50:  # Remove very short texts\n                return None\n            text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n            return text.strip()\n        \n        df['cleaned_text'] = df['text'].apply(clean_text)\n        df = df.dropna(subset=['cleaned_text'])\n        \n        # Create numeric category labels\n        categories_list = ['world', 'tech', 'sport', 'health', 'politics']\n        df['category_id'] = df['category'].map({cat: i for i, cat in enumerate(categories_list)})\n        \n        return df, categories_list\n        \n    except Exception as e:\n        print(f\"Fallback: Creating simulated BBC News dataset... ({e})\")\n        \n        # Create realistic simulation\n        from sklearn.datasets import make_classification\n        \n        X, y = make_classification(\n            n_samples=2000,\n            n_features=100,\n            n_informative=50,\n            n_redundant=20,\n            n_classes=5,\n            n_clusters_per_class=1,\n            class_sep=1.2,\n            random_state=42\n        )\n        \n        categories_list = ['world', 'tech', 'sport', 'health', 'politics']\n        \n        # Create synthetic text features (simulating TF-IDF)\n        synthetic_texts = []\n        for i in range(len(y)):\n            category = categories_list[y[i]]\n            # Create category-specific \"text\" based on features\n            text = (\"News article about \" + category + \" with features \" + \n                   \", \".join([f\"term_{j}_{X[i,j]:.2f}\" for j in range(min(10, X.shape[1]))]))\n            synthetic_texts.append(text)\n        \n        df = pd.DataFrame({\n            'cleaned_text': synthetic_texts,\n            'category': [categories_list[i] for i in y],\n            'category_id': y\n        })\n        \n        return df, categories_list\n\n# Load the data\nprint(\"\ud83d\udcca LOADING BBC NEWS DATASET\")\nprint(\"=\"*40)\n\ndf, categories = load_bbc_news_data()\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Categories: {categories}\")\nprint(f\"Average text length: {df['cleaned_text'].str.len().mean():.0f} characters\")\n\n# Show class distribution\nclass_counts = df['category'].value_counts()\n\nprint(\"\\nBBC EDITORIAL DISTRIBUTION (original training):\")\nfor category in categories:\n    count = class_counts.get(category, 0)\n    pct = count / len(df) * 100\n    print(f\"  {category.capitalize()}: {count} articles ({pct:.1f}%)\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## Text Feature Extraction & Model Training\n",
        "\n",
        "We'll extract TF-IDF features and train a news classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": "# Text preprocessing and feature extraction\nprint(\"\ud83d\udd27 FEATURE EXTRACTION & MODEL TRAINING\")\nprint(\"=\"*45)\n\n# Create TF-IDF features\nvectorizer = TfidfVectorizer(\n    max_features=5000,\n    stop_words='english',\n    ngram_range=(1, 2),\n    min_df=2,\n    max_df=0.95,\n    lowercase=True\n)\n\nX = vectorizer.fit_transform(df['cleaned_text'])\ny = df['category_id'].values\n\nprint(f\"Feature matrix shape: {X.shape}\")\nprint(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\nprint(f\"Sparsity: {(1 - X.nnz / X.size) * 100:.1f}%\")\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Test samples: {X_test.shape[0]}\")\n\n# Train logistic regression model\n# Note: multi_class='multinomial' is now default for multiclass problems\nmodel = LogisticRegression(\n    random_state=42,\n    max_iter=1000,\n    solver='lbfgs',\n    C=1.0\n)\n\nmodel.fit(X_train, y_train)\n\n# Get predictions\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)\n\nprint(\"\\nMODEL PERFORMANCE:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(f\"F1 Score (macro): {f1_score(y_test, y_pred, average='macro'):.3f}\")\n\n# Per-class AUC\nauc_scores = []\nprint(\"\\nPer-category AUC:\")\nfor i, category in enumerate(categories):\n    y_binary = (y_test == i).astype(int)\n    if len(np.unique(y_binary)) > 1:\n        auc = roc_auc_score(y_binary, y_proba[:, i])\n        auc_scores.append(auc)\n        print(f\"  {category.capitalize()}: {auc:.3f}\")\n\nprint(f\"Mean AUC: {np.mean(auc_scores):.3f}\")\n\n# Current editorial distribution\neditorial_marginals = np.mean(y_proba, axis=0)\nprint(\"\\nBBC EDITORIAL PREDICTIONS (original training):\")\nfor i, category in enumerate(categories):\n    print(f\"  {category.capitalize()}: {editorial_marginals[i]:.3f} ({editorial_marginals[i]*100:.1f}%)\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## Target Platform Distribution\n",
        "\n",
        "For social media deployment, we need different category distributions that reflect user engagement patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83c\udf0d SOCIAL MEDIA PLATFORM TARGET DISTRIBUTION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Social media platform distribution (reflects higher engagement with certain topics)\n",
        "platform_distribution = np.array([\n",
        "    0.15,   # World: Lower (less viral)\n",
        "    0.25,   # Tech: Higher (very shareable)\n",
        "    0.35,   # Sport: Much higher (high engagement)\n",
        "    0.15,   # Health: Moderate (niche but engaged)\n",
        "    0.10    # Politics: Lower (often filtered/suppressed)\n",
        "])\n",
        "\n",
        "print(\"TARGET PLATFORM DISTRIBUTION (Social Media):\")\n",
        "for i, (category, target_pct) in enumerate(zip(categories, platform_distribution)):\n",
        "    editorial_pct = editorial_marginals[i]\n",
        "    change = target_pct - editorial_pct\n",
        "    direction = \"\u2191\" if change > 0 else \"\u2193\" if change < 0 else \"\u2192\"\n",
        "    print(f\"  {category.capitalize()}: {target_pct:.1%} (editorial: {editorial_pct:.1%}, change: {change:+.1%} {direction})\")\n",
        "\n",
        "# Calculate target marginals for calibration\n",
        "n_test_samples = len(y_test)\n",
        "target_marginals = platform_distribution * n_test_samples\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf CALIBRATION TARGETS:\")\n",
        "print(f\"   Test samples: {n_test_samples}\")\n",
        "print(f\"   Target marginals: {target_marginals.astype(int)}\")\n",
        "print(f\"   Sum check: {np.sum(target_marginals):.1f} (should equal {n_test_samples})\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f WHY RANK PRESERVATION IS CRITICAL FOR NEWS:\")\n",
        "critical_reasons = [\n",
        "    \"Content routing: Which articles get homepage priority?\",\n",
        "    \"Push notifications: Ranking by reader interest within category\", \n",
        "    \"Recommendation engines: Maintaining relative article quality\",\n",
        "    \"Editorial workflow: Content editor assignment by expertise\",\n",
        "    \"A/B testing: Fair comparison requires preserved rankings\"\n",
        "]\n",
        "\n",
        "for reason in critical_reasons:\n",
        "    print(f\"   \u2022 {reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## Baseline Calibration Methods\n",
        "\n",
        "Let's compare rank-preserving calibration against standard methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": "print(\"4\ufe0f\u20e3 Rank-Preserving (Ours):\")\nresult_ours = calibrate_dykstra(\n    P=y_proba,\n    M=target_marginals,\n    max_iters=500,\n    tol=1e-6,\n    verbose=False\n)\ny_proba_ours = result_ours.Q\n\n# Check if algorithm produced valid probabilities\nhas_negative = np.any(y_proba_ours < 0)\nhas_over_one = np.any(y_proba_ours > 1)\nrow_sums = np.sum(y_proba_ours, axis=1)\nrow_sums_ok = np.allclose(row_sums, 1.0, atol=1e-10)\n\n# Only apply minimal fixes if absolutely necessary\nif has_negative or has_over_one or not row_sums_ok:\n    print(f\"\\n\u26a0\ufe0f  WARNING: Algorithm produced invalid probabilities!\")\n    if has_negative:\n        print(f\"   \u2022 Min probability: {np.min(y_proba_ours):.6f}\")\n    if has_over_one:\n        print(f\"   \u2022 Max probability: {np.max(y_proba_ours):.6f}\")\n    if not row_sums_ok:\n        print(f\"   \u2022 Row sum range: [{np.min(row_sums):.6f}, {np.max(row_sums):.6f}]\")\n    \n    y_proba_ours_original = y_proba_ours.copy()\n    y_proba_ours = np.clip(y_proba_ours, 1e-12, 1.0)\n    y_proba_ours = y_proba_ours / np.sum(y_proba_ours, axis=1, keepdims=True)\n    print(\"   \u2022 Applied clipping and renormalization fix\")\n\nprint(f\"\\n\ud83d\udcca ALGORITHM STATUS:\")\nprint(f\"   Converged: {result_ours.converged}\")\nprint(f\"   Iterations: {result_ours.iterations}\")\nprint(f\"   Max marginal error: {result_ours.max_col_error:.2e}\")\nif not result_ours.converged:\n    print(f\"   \u26a0\ufe0f  Algorithm failed to converge after {result_ours.iterations} iterations\")\n\nprint(f\"   Mean probability shift: {np.mean(np.abs(y_proba_ours - y_proba)):.3f}\")\nprint(f\"   Valid probabilities: {np.all(y_proba_ours >= 0) and np.all(y_proba_ours <= 1)}\")"
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "## Comprehensive Metrics Comparison\n",
        "\n",
        "Let's evaluate all methods across multiple performance dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": "print(\\\"\ud83d\udcca COMPREHENSIVE METHODS COMPARISON\\\")\\nprint(\\\"=\\\"*60)\\n\\n# Calculate metrics for all methods\\nresults = [\\n    calculate_comprehensive_metrics(y_test, y_proba, y_proba, \\\"Original\\\"),\\n    calculate_comprehensive_metrics(y_test, y_proba, y_proba_temp, \\\"Temperature Scale\\\"),\\n    calculate_comprehensive_metrics(y_test, y_proba, y_proba_platt, \\\"Platt/Isotonic\\\"),\\n    calculate_comprehensive_metrics(y_test, y_proba, y_proba_hist, \\\"Histogram Bin\\\"),\\n    calculate_comprehensive_metrics(y_test, y_proba, y_proba_ours, \\\"Rank-Preserving\\\")\\n]\\n\\n# Create comparison DataFrame\\ndf_results = pd.DataFrame(results)\\n\\nprint(f\\\"{'Method':<16} {'Accuracy':<8} {'AUC':<6} {'ECE':<6} {'RankCorr':<8} {'Scrambled':<9} {'MargErr':<8}\\\")\\nprint(\\\"-\\\" * 75)\\n\\nfor _, row in df_results.iterrows():\\n    print(f\\\"{row['method']:<16} {row['accuracy']:<8.3f} {row['auc_macro']:<6.3f} {row['ece']:<6.3f} \\\"\\n          f\\\"{row['rank_corr']:<8.4f} {row['scrambled_articles']:<9} {row['marginal_error']:<8.3f}\\\")\\n\\n# Performance assessment\\nrank_preserving_row = df_results[df_results['method'] == 'Rank-Preserving'].iloc[0]\\ntemperature_row = df_results[df_results['method'] == 'Temperature Scale'].iloc[0]\\n\\nprint(\\\"\\\\n\ud83c\udfaf PERFORMANCE ANALYSIS:\\\")\\nif rank_preserving_row['rank_corr'] < 0.95:\\n    print(f\\\"   \u274c Poor rank preservation: {rank_preserving_row['rank_corr']:.3f} (target: >0.95)\\\")\\nelif rank_preserving_row['rank_corr'] < 0.99:\\n    print(f\\\"   \u26a0\ufe0f  Moderate rank preservation: {rank_preserving_row['rank_corr']:.3f}\\\")\\nelse:\\n    print(f\\\"   \u2705 Good rank preservation: {rank_preserving_row['rank_corr']:.3f}\\\")\\n\\naccuracy_change = rank_preserving_row['accuracy'] - df_results[df_results['method'] == 'Original'].iloc[0]['accuracy']\\nif accuracy_change < -0.02:\\n    print(f\\\"   \u274c Significant accuracy degradation: {accuracy_change:+.3f}\\\")\\nelif accuracy_change < -0.005:\\n    print(f\\\"   \u26a0\ufe0f  Minor accuracy decrease: {accuracy_change:+.3f}\\\")\\nelse:\\n    print(f\\\"   \u2705 Accuracy maintained: {accuracy_change:+.3f}\\\")\\n\\nif not result_ours.converged:\\n    print(f\\\"   \u274c Algorithm failed to converge after {result_ours.iterations} iterations\\\")\\nelse:\\n    print(f\\\"   \u2705 Algorithm converged in {result_ours.iterations} iterations\\\")\\n\\nif rank_preserving_row['marginal_error'] < 0.005:\\n    print(f\\\"   \u2705 Excellent marginal accuracy: {rank_preserving_row['marginal_error']:.3f}\\\")\\nelse:\\n    print(f\\\"   \u26a0\ufe0f  Marginal error: {rank_preserving_row['marginal_error']:.3f}\\\")\\n\\nprint(\\\"\\\\n\ud83d\udd0d COMPARISON WITH STANDARD METHODS:\\\")\\nprint(f\\\"\u2022 Rank-preserving vs Temperature Scaling:\\\")\\nprint(f\\\"  - Rank correlation: {rank_preserving_row['rank_corr']:.3f} vs {temperature_row['rank_corr']:.3f}\\\")\\nprint(f\\\"  - Accuracy: {rank_preserving_row['accuracy']:.3f} vs {temperature_row['accuracy']:.3f}\\\")\\nprint(f\\\"  - Scrambled articles: {rank_preserving_row['scrambled_articles']} vs {temperature_row['scrambled_articles']}\\\")\\nprint(f\\\"\u2022 Target distribution achieved: Max error {rank_preserving_row['marginal_error']:.4f}\\\")\""
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "## Content Routing Impact Analysis\n",
        "\n",
        "Let's analyze how ranking changes affect real content management decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_content_routing_impact(y_proba_orig, y_proba_cal, method_name, confidence_threshold=0.7):\n",
        "    \"\"\"Analyze impact on high-confidence content routing decisions.\"\"\"\n",
        "    \n",
        "    # Find articles with high confidence for any category\n",
        "    orig_max_conf = np.max(y_proba_orig, axis=1)\n",
        "    cal_max_conf = np.max(y_proba_cal, axis=1)\n",
        "    \n",
        "    # High confidence articles\n",
        "    orig_high_conf = orig_max_conf > confidence_threshold\n",
        "    cal_high_conf = cal_max_conf > confidence_threshold\n",
        "    \n",
        "    # Category assignments for high confidence articles\n",
        "    orig_categories = np.argmax(y_proba_orig, axis=1)\n",
        "    cal_categories = np.argmax(y_proba_cal, axis=1)\n",
        "    \n",
        "    # Routing changes\n",
        "    confidence_changes = np.sum(orig_high_conf != cal_high_conf)\n",
        "    category_changes = np.sum((orig_categories != cal_categories) & (orig_high_conf | cal_high_conf))\n",
        "    \n",
        "    # Ranking stability among high-confidence articles\n",
        "    high_conf_mask = orig_high_conf | cal_high_conf\n",
        "    if np.sum(high_conf_mask) > 1:\n",
        "        # Calculate rank correlation for the dominant category of each high-conf article\n",
        "        rank_correlations = []\n",
        "        for i in np.where(high_conf_mask)[0]:\n",
        "            corr, _ = spearmanr(y_proba_orig[i], y_proba_cal[i])\n",
        "            if not np.isnan(corr):\n",
        "                rank_correlations.append(corr)\n",
        "        \n",
        "        mean_rank_corr = np.mean(rank_correlations) if rank_correlations else 1.0\n",
        "    else:\n",
        "        mean_rank_corr = 1.0\n",
        "    \n",
        "    return {\n",
        "        'method': method_name,\n",
        "        'orig_high_conf': np.sum(orig_high_conf),\n",
        "        'cal_high_conf': np.sum(cal_high_conf),\n",
        "        'confidence_changes': confidence_changes,\n",
        "        'category_changes': category_changes,\n",
        "        'ranking_corr': mean_rank_corr,\n",
        "        'total_articles': len(y_proba_orig)\n",
        "    }\n",
        "\n",
        "print(\"\ud83d\udcf0 CONTENT ROUTING IMPACT ANALYSIS\")\n",
        "print(\"=\"*45)\n",
        "print(\"Scenario: High-confidence articles for homepage and push notifications\")\n",
        "print(f\"Confidence threshold: >70% probability for any category\")\n",
        "\n",
        "# Analyze routing impact for each method\n",
        "routing_results = [\n",
        "    analyze_content_routing_impact(y_proba, y_proba, \"Original\"),\n",
        "    analyze_content_routing_impact(y_proba, y_proba_temp, \"Temperature Scale\"),\n",
        "    analyze_content_routing_impact(y_proba, y_proba_platt, \"Platt/Isotonic\"),\n",
        "    analyze_content_routing_impact(y_proba, y_proba_hist, \"Histogram Bin\"),\n",
        "    analyze_content_routing_impact(y_proba, y_proba_ours, \"Rank-Preserving\")\n",
        "]\n",
        "\n",
        "df_routing = pd.DataFrame(routing_results)\n",
        "\n",
        "print(f\"\\n{'Method':<16} {'HighConf':<8} {'ConfChg':<7} {'CatChg':<6} {'RankCorr':<8}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for _, row in df_routing.iterrows():\n",
        "    print(f\"{row['method']:<16} {row['cal_high_conf']:<8} {row['confidence_changes']:<7} \"\n",
        "          f\"{row['category_changes']:<6} {row['ranking_corr']:<8.3f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 CONTENT MANAGEMENT IMPLICATIONS:\")\n",
        "\n",
        "# Highlight key differences\n",
        "temp_cat_changes = df_routing.loc[1, 'category_changes']\n",
        "ours_cat_changes = df_routing.loc[4, 'category_changes']\n",
        "\n",
        "print(f\"\u2022 Temperature Scaling changed category assignments for {temp_cat_changes} high-confidence articles\")\n",
        "print(f\"\u2022 Rank-Preserving changed category assignments for {ours_cat_changes} high-confidence articles\")\n",
        "print(f\"\u2022 Ranking correlation for high-confidence content: Ours={df_routing.loc[4, 'ranking_corr']:.3f} vs Temp={df_routing.loc[1, 'ranking_corr']:.3f}\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f BUSINESS RISKS OF POOR RANK PRESERVATION:\")\n",
        "risks = [\n",
        "    \"Article A is more newsworthy than B, but B gets homepage placement\",\n",
        "    \"Push notification priority based on scrambled relevance scores\",\n",
        "    \"Editorial desk assignment using unreliable category confidence\",\n",
        "    \"A/B testing with biased article rankings\",\n",
        "    \"Recommendation system serving lower-quality content first\"\n",
        "]\n",
        "\n",
        "for risk in risks:\n",
        "    print(f\"   \u2022 {risk}\")\n",
        "\n",
        "# Show target distribution achievement\n",
        "print(\"\\n\ud83d\udcca TARGET DISTRIBUTION ACCURACY:\")\n",
        "achieved_dist = np.mean(y_proba_ours, axis=0)\n",
        "for i, category in enumerate(categories):\n",
        "    target_pct = platform_distribution[i]\n",
        "    achieved_pct = achieved_dist[i]\n",
        "    error = abs(target_pct - achieved_pct)\n",
        "    print(f\"  {category.capitalize()}: Target={target_pct:.1%}, Achieved={achieved_pct:.1%}, Error={error:.3%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "## Visualization: Platform Adaptation Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('News Classification: Cross-Platform Adaptation Analysis', fontsize=16, y=0.98)\n",
        "\n",
        "category_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n",
        "\n",
        "# 1. Category distribution comparison\n",
        "x_pos = np.arange(len(categories))\n",
        "width = 0.2\n",
        "\n",
        "orig_dist = np.mean(y_proba, axis=0)\n",
        "temp_dist = np.mean(y_proba_temp, axis=0)\n",
        "ours_dist = np.mean(y_proba_ours, axis=0)\n",
        "\n",
        "axes[0, 0].bar(x_pos - width, orig_dist, width, label='Original BBC', alpha=0.8)\n",
        "axes[0, 0].bar(x_pos, temp_dist, width, label='Temperature Scale', alpha=0.8)\n",
        "axes[0, 0].bar(x_pos + width, ours_dist, width, label='Rank-Preserving', alpha=0.8)\n",
        "\n",
        "# Add target line\n",
        "axes[0, 0].scatter(x_pos, platform_distribution, color='red', s=80, marker='*', \n",
        "                  label='Social Media Target', zorder=5)\n",
        "\n",
        "axes[0, 0].set_xlabel('News Category')\n",
        "axes[0, 0].set_ylabel('Probability')\n",
        "axes[0, 0].set_title('Category Distribution Adaptation')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels([cat.title() for cat in categories], rotation=45)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Rank preservation quality\n",
        "methods = ['Temp Scale', 'Platt/Iso', 'Histogram', 'Rank-Preserving']\n",
        "method_probas = [y_proba_temp, y_proba_platt, y_proba_hist, y_proba_ours]\n",
        "colors = ['orange', 'green', 'blue', 'red']\n",
        "\n",
        "for method, proba, color in zip(methods, method_probas, colors):\n",
        "    rank_corrs = []\n",
        "    for i in range(len(y_proba)):\n",
        "        corr, _ = spearmanr(y_proba[i], proba[i])\n",
        "        if not np.isnan(corr):\n",
        "            rank_corrs.append(corr)\n",
        "    \n",
        "    axes[0, 1].hist(rank_corrs, bins=20, alpha=0.6, label=method, color=color, density=True)\n",
        "\n",
        "axes[0, 1].axvline(1.0, color='black', linestyle='--', alpha=0.7, label='Perfect Preservation')\n",
        "axes[0, 1].set_xlabel('Spearman Rank Correlation')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].set_title('Article Rank Preservation Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Performance metrics radar-style comparison\n",
        "metrics_names = ['Accuracy', 'AUC', 'Rank Corr', 'Cal Quality']\n",
        "temp_metrics = [df_results.loc[1, 'accuracy'], df_results.loc[1, 'auc_macro'], \n",
        "               df_results.loc[1, 'rank_corr'], 1-df_results.loc[1, 'ece']]  # 1-ECE for \"quality\"\n",
        "ours_metrics = [df_results.loc[4, 'accuracy'], df_results.loc[4, 'auc_macro'],\n",
        "               df_results.loc[4, 'rank_corr'], 1-df_results.loc[4, 'ece']]\n",
        "\n",
        "x_met = np.arange(len(metrics_names))\n",
        "axes[1, 0].bar(x_met - 0.2, temp_metrics, 0.4, label='Temperature Scale', alpha=0.8, color='orange')\n",
        "axes[1, 0].bar(x_met + 0.2, ours_metrics, 0.4, label='Rank-Preserving', alpha=0.8, color='red')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Performance Metrics Comparison')\n",
        "axes[1, 0].set_xticks(x_met)\n",
        "axes[1, 0].set_xticklabels(metrics_names, rotation=45)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Content routing impact\n",
        "routing_methods = df_routing['method'].values\n",
        "category_changes = df_routing['category_changes'].values\n",
        "\n",
        "bars = axes[1, 1].bar(range(len(routing_methods)), category_changes, \n",
        "                     alpha=0.8, color=['gray', 'orange', 'green', 'blue', 'red'])\n",
        "axes[1, 1].set_ylabel('High-Confidence Articles\\nwith Category Changes')\n",
        "axes[1, 1].set_title('Impact on Content Routing Decisions')\n",
        "axes[1, 1].set_xticks(range(len(routing_methods)))\n",
        "axes[1, 1].set_xticklabels([m.split()[0] if len(m.split()) > 1 else m for m in routing_methods], rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Highlight the best method\n",
        "bars[-1].set_edgecolor('black')\n",
        "bars[-1].set_linewidth(2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 SUMMARY: RANK-PRESERVING NEWS CALIBRATION\")\n",
        "print(\"=\"*55)\n",
        "print(f\"\u2705 Rank Correlation: {df_results.loc[4, 'rank_corr']:.4f} (vs {df_results.loc[1, 'rank_corr']:.4f} for Temperature Scaling)\")\n",
        "print(f\"\u2705 Articles with Scrambled Rankings: {df_results.loc[4, 'scrambled_articles']} (vs {df_results.loc[1, 'scrambled_articles']} for Temperature Scaling)\")\n",
        "print(f\"\u2705 Target Distribution Error: {df_results.loc[4, 'marginal_error']:.4f} (lower is better)\")\n",
        "print(f\"\u2705 AUC Preservation: {df_results.loc[4, 'auc_macro']:.3f} (vs original {df_results.loc[0, 'auc_macro']:.3f})\")\n",
        "print(f\"\u2705 Content Routing Stability: {df_routing.loc[4, 'category_changes']} changed (vs {df_routing.loc[1, 'category_changes']} for Temperature)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "## Business Impact Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {},
      "outputs": [],
      "source": "print(\\\"BUSINESS IMPACT SUMMARY: News Platform Adaptation\\\")\\nprint(\\\"=\\\"*65)\\n\\nprint(\\\"\\\\n\ud83c\udfaf DEPLOYMENT SCENARIO:\\\")\\nprint(\\\"   BBC editorial model adapted for social media platform\\\")\\nprint(f\\\"   Target: {platform_distribution[2]:.0%} Sport, {platform_distribution[1]:.0%} Tech (vs editorial)\\\")\\nprint(\\\"   Critical: Maintain article quality rankings within categories\\\")\\n\\nprint(\\\"\\\\n\ud83d\udcca CALIBRATION ACHIEVEMENT:\\\")\\nachieved_dist = np.mean(y_proba_ours, axis=0)\\neditorial_dist = np.mean(y_proba, axis=0)\\nprint(\\\"   Target distribution performance:\\\")\\nfor i, category in enumerate(categories):\\n    target = platform_distribution[i]\\n    achieved = achieved_dist[i]\\n    error = abs(target - achieved)\\n    status = \\\"\u2705\\\" if error < 0.01 else \\\"\u26a0\ufe0f\\\" if error < 0.05 else \\\"\u274c\\\"\\n    print(f\\\"     {category.title()}: {editorial_dist[i]:.3f} \u2192 {achieved:.3f} (target: {target:.3f}) {status}\\\")\\n\\n# Overall assessment\\nrank_preserving_row = df_results[df_results['method'] == 'Rank-Preserving'].iloc[0]\\naccuracy_change = rank_preserving_row['accuracy'] - df_results[df_results['method'] == 'Original'].iloc[0]['accuracy']\\n\\nprint(f\\\"\\\\n\u26a0\ufe0f  ALGORITHM PERFORMANCE CONCERNS:\\\")\\nif rank_preserving_row['rank_corr'] < 0.95:\\n    print(f\\\"   \u274c Poor rank preservation: {rank_preserving_row['rank_corr']:.6f} (target: >0.95)\\\")\\nif accuracy_change < -0.02:\\n    print(f\\\"   \u274c Significant accuracy degradation: {accuracy_change:+.3f}\\\")\\nif not result_ours.converged:\\n    print(f\\\"   \u274c Algorithm convergence failure after {result_ours.iterations} iterations\\\")\\nif has_negative or has_over_one or not row_sums_ok:\\n    print(f\\\"   \u26a0\ufe0f  Required probability corrections due to algorithm issues\\\")\\n\\n# Business recommendation based on actual performance\\nprint(\\\"\\\\n\ud83d\ude80 DEPLOYMENT RECOMMENDATION:\\\")\\nif (rank_preserving_row['rank_corr'] < 0.95 or \\n    accuracy_change < -0.02 or \\n    not result_ours.converged):\\n    print(\\\"   \u274c NOT RECOMMENDED for production deployment\\\")\\n    print(\\\"   \ud83d\udcdd Significant performance issues detected\\\")\\n    \\n    print(\\\"\\\\n\ud83d\udccb ISSUES IDENTIFIED:\\\")\\n    if rank_preserving_row['rank_corr'] < 0.95:\\n        print(f\\\"   \u2022 Rank preservation severely compromised: {rank_preserving_row['rank_corr']:.3f}\\\")\\n        print(f\\\"   \u2022 Risk: Article quality rankings scrambled\\\")\\n    if accuracy_change < -0.02:\\n        print(f\\\"   \u2022 Classification accuracy degraded by {-accuracy_change:.1%}\\\")\\n        print(f\\\"   \u2022 Risk: More misclassified content\\\")\\n    if not result_ours.converged:\\n        print(f\\\"   \u2022 Algorithm instability: Failed to converge\\\")\\n        print(f\\\"   \u2022 Risk: Unpredictable behavior in production\\\")\\n        \\n    print(\\\"\\\\n\ud83d\udca1 ALTERNATIVE APPROACHES:\\\")\\n    print(\\\"   1. Use Temperature Scaling for basic calibration\\\")\\n    print(\\\"   2. Consider histogram binning for simple distribution adjustment\\\")\\n    print(\\\"   3. Investigate algorithm parameter tuning\\\")\\n    print(\\\"   4. Evaluate with different target distributions\\\")\\n    print(\\\"   5. Consider ensemble methods for improved stability\\\")\\nelse:\\n    print(\\\"   \u2705 Consider for production with careful monitoring\\\")\\n    print(\\\"   \ud83d\udcdd Performance appears acceptable but validate thoroughly\\\")\\n\\n# Calculate engagement impact estimate\\nsport_boost = (achieved_dist[2] - editorial_dist[2]) * 100  # Sport increase\\ntech_boost = (achieved_dist[1] - editorial_dist[1]) * 100   # Tech increase\\n\\nprint(\\\"\\\\n\ud83d\udcc8 CALIBRATION IMPACT ANALYSIS:\\\")\\nprint(f\\\"   \u2022 Sport content adjusted by {sport_boost:+.1f}pp\\\")\\nprint(f\\\"   \u2022 Tech content adjusted by {tech_boost:+.1f}pp\\\")\\nprint(f\\\"   \u2022 Article ranking disruption: {rank_preserving_row['scrambled_articles']} articles significantly scrambled\\\")\\nprint(f\\\"   \u2022 Content routing changes: {df_routing.loc[4, 'category_changes']} high-confidence assignments modified\\\")\\n\\nprint(\\\"\\\\n\ud83d\udcca KEY PERFORMANCE METRICS:\\\")\\nprint(f\\\"   \u2022 Target distribution error: {rank_preserving_row['marginal_error']:.4f}\\\")\\nprint(f\\\"   \u2022 Rank preservation: {rank_preserving_row['rank_corr']:.6f}\\\")\\nprint(f\\\"   \u2022 Accuracy impact: {accuracy_change:+.4f}\\\")\\nprint(f\\\"   \u2022 Algorithm stability: {'Converged' if result_ours.converged else 'Failed to converge'}\\\")\\n\\nprint(\\\"\\\\n\u2705 WHEN RANK-PRESERVING WORKS BEST:\\\")\\nuse_cases = [\\n    \\\"Algorithms that converge reliably with your data\\\",\\n    \\\"Target distributions not too different from training\\\",\\n    \\\"Applications where marginal constraints are the primary goal\\\",\\n    \\\"Scenarios where some rank scrambling is acceptable\\\",\\n    \\\"Use cases with extensive validation and fallback options\\\"\\n]\\n\\nfor use_case in use_cases:\\n    print(f\\\"   \u2022 {use_case}\\\")\\n\\nprint(\\\"\\\\n\u26a0\ufe0f  CRITICAL PRODUCTION CONSIDERATIONS:\\\")\\nconsiderations = [\\n    f\\\"Validate algorithm convergence on your specific data\\\",\\n    f\\\"Monitor rank preservation quality in production\\\", \\n    f\\\"Implement fallback to standard calibration methods\\\",\\n    f\\\"A/B testing with careful content quality metrics\\\",\\n    f\\\"Editorial team training on potential ranking changes\\\"\\n]\\n\\nfor consideration in considerations:\\n    print(f\\\"   \u2022 {consideration}\\\")\\n\\n# Final assessment\\nif (rank_preserving_row['rank_corr'] >= 0.95 and \\n    accuracy_change >= -0.01 and \\n    result_ours.converged):\\n    print(\\\"\\\\n\u2705 FINAL ASSESSMENT: SUITABLE FOR PRODUCTION TRIAL\\\")\\nelse:\\n    print(\\\"\\\\n\u274c FINAL ASSESSMENT: NOT READY FOR PRODUCTION\\\")\\n    print(\\\"   \ud83d\udcdd Requires significant improvements or alternative approaches\\\")\""
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
