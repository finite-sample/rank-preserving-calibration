{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Risk Assessment: Population Deployment with Rank Preservation\n",
    "\n",
    "**Problem**: A cardiovascular risk model trained on clinical trial data needs to be deployed for population screening. Clinical trials over-represent severe cases, so the model's risk probabilities need adjustment to match the general population's disease distribution - but critically, **patient risk rankings must be preserved** for proper triage.\n",
    "\n",
    "## Unique Value Proposition\n",
    "\n",
    "This example demonstrates why **rank-preserving calibration** is essential in medical applications:\n",
    "\n",
    "- ðŸ¥ **Clinical triage depends on relative risk rankings** between patients\n",
    "- ðŸ“Š **Population estimates need accurate marginal distributions**  \n",
    "- âš ï¸ **Standard calibration methods can scramble patient orderings**\n",
    "- âœ… **Our method preserves rankings while adjusting population rates**\n",
    "\n",
    "We'll use the **UCI Heart Disease dataset** - real clinical data with documented population vs. clinical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Import our calibration package\n",
    "from rank_preserving_calibration import calibrate_dykstra\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette([\"#e74c3c\", \"#f39c12\", \"#3498db\", \"#2ecc71\", \"#9b59b6\"])\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸ¥ MEDICAL RISK CALIBRATION WITH REAL DATA\")\n",
    "print(\"Focus: Population deployment with rank preservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UCI Heart Disease Dataset\n",
    "\n",
    "We'll use the famous UCI Heart Disease dataset, which contains real clinical measurements from patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heart_disease_data():\n",
    "    \"\"\"Load and preprocess UCI Heart Disease dataset.\"\"\"\n",
    "    # Heart disease data (we'll fetch from UCI or use sklearn's make_classification to simulate real patterns)\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    \n",
    "    try:\n",
    "        # Try to load real heart disease data from OpenML\n",
    "        heart_data = fetch_openml(name='heart-disease', version=1, as_frame=True, parser='auto')\n",
    "        X = heart_data.data\n",
    "        y = heart_data.target\n",
    "        \n",
    "        # Convert target to numeric if needed\n",
    "        if y.dtype == 'object':\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "            \n",
    "    except:\n",
    "        # Fallback: Create realistic heart disease simulation\n",
    "        print(\"Creating realistic heart disease simulation...\")\n",
    "        from sklearn.datasets import make_classification\n",
    "        \n",
    "        # Create a realistic 5-class heart disease severity dataset\n",
    "        X, y = make_classification(\n",
    "            n_samples=1000,\n",
    "            n_features=13,  # Similar to actual heart disease features\n",
    "            n_informative=10,\n",
    "            n_redundant=3,\n",
    "            n_classes=5,  # 0: No disease, 1-4: Increasing severity\n",
    "            n_clusters_per_class=1,\n",
    "            class_sep=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create realistic feature names\n",
    "        feature_names = [\n",
    "            'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', \n",
    "            'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'\n",
    "        ]\n",
    "        \n",
    "        X = pd.DataFrame(X, columns=feature_names)\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Load the data\n",
    "print(\"ðŸ“Š LOADING UCI HEART DISEASE DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X, y = load_heart_disease_data()\n",
    "\n",
    "# Ensure we have 5 severity classes (0=none, 1-4=increasing severity)\n",
    "if len(np.unique(y)) != 5:\n",
    "    # Bin into 5 severity classes if needed\n",
    "    y = pd.cut(y, bins=5, labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {list(X.columns)[:5]}...\")\n",
    "print(f\"Target classes: {sorted(np.unique(y))}\")\n",
    "\n",
    "# Show class distribution\n",
    "class_counts = np.bincount(y)\n",
    "severity_labels = ['No Disease', 'Mild', 'Moderate', 'Severe', 'Critical']\n",
    "\n",
    "print(\"\\nCLINICAL TRIAL DISTRIBUTION (biased toward severe cases):\")\n",
    "for i, (label, count) in enumerate(zip(severity_labels, class_counts)):\n",
    "    pct = count / len(y) * 100\n",
    "    print(f\"  {label}: {count} patients ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Clinical Trial Bias\n",
    "\n",
    "We'll train a cardiovascular risk model and simulate the bias present in clinical trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– TRAINING CARDIOVASCULAR RISK MODEL\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Train Random Forest classifier\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"Model accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "\n",
    "# Current clinical trial marginals\n",
    "clinical_marginals = np.mean(y_proba, axis=0)\n",
    "print(\"\\nCLINICAL TRIAL PREDICTIONS (biased):\")\n",
    "for i, (label, prob) in enumerate(zip(severity_labels, clinical_marginals)):\n",
    "    print(f\"  {label}: {prob:.3f} ({prob*100:.1f}%)\")\n",
    "\n",
    "# Multi-class AUC\n",
    "auc_scores = []\n",
    "for i in range(len(severity_labels)):\n",
    "    if len(np.unique(y_test == i)) > 1:  # Only if both classes exist\n",
    "        y_binary = (y_test == i).astype(int)\n",
    "        auc = roc_auc_score(y_binary, y_proba[:, i])\n",
    "        auc_scores.append(auc)\n",
    "        print(f\"AUC {severity_labels[i]}: {auc:.3f}\")\n",
    "\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Health Target Distribution\n",
    "\n",
    "For population deployment, we need to match real-world cardiovascular disease prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŒ POPULATION HEALTH TARGET DISTRIBUTION\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Real-world population distribution (based on cardiovascular epidemiology)\n",
    "population_distribution = np.array([\n",
    "    0.75,   # No Disease: Most of population is healthy\n",
    "    0.12,   # Mild: Some risk factors\n",
    "    0.08,   # Moderate: Moderate risk\n",
    "    0.04,   # Severe: High risk\n",
    "    0.01    # Critical: Very high risk\n",
    "])\n",
    "\n",
    "print(\"POPULATION SCREENING TARGET DISTRIBUTION:\")\n",
    "for i, (label, target_pct) in enumerate(zip(severity_labels, population_distribution)):\n",
    "    clinical_pct = clinical_marginals[i]\n",
    "    change = target_pct - clinical_pct\n",
    "    direction = \"â†‘\" if change > 0 else \"â†“\" if change < 0 else \"â†’\"\n",
    "    print(f\"  {label}: {target_pct:.1%} (clinical: {clinical_pct:.1%}, change: {change:+.1%} {direction})\")\n",
    "\n",
    "# Calculate target marginals for calibration\n",
    "n_test_samples = len(y_test)\n",
    "target_marginals = population_distribution * n_test_samples\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CALIBRATION TARGETS:\")\n",
    "print(f\"   Test samples: {n_test_samples}\")\n",
    "print(f\"   Target marginals: {target_marginals.astype(int)}\")\n",
    "print(f\"   Sum check: {np.sum(target_marginals):.1f} (should equal {n_test_samples})\")\n",
    "\n",
    "print(\"\\nâš ï¸ WHY RANK PRESERVATION IS CRITICAL:\")\n",
    "critical_reasons = [\n",
    "    \"Patient triage: Who gets priority for specialist referral?\",\n",
    "    \"Treatment decisions: Medication intensity based on relative risk\",\n",
    "    \"Resource allocation: ICU beds, cardiac procedures, preventive care\",\n",
    "    \"Clinical trials: Patient stratification for drug studies\",\n",
    "    \"Insurance: Risk-based premium calculations\"\n",
    "]\n",
    "\n",
    "for reason in critical_reasons:\n",
    "    print(f\"   â€¢ {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Methods Comparison\n",
    "\n",
    "We'll compare rank-preserving calibration against standard methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.isotonic import IsotonicRegression\n\ndef temperature_scaling(y_proba, y_true):\n    \"\"\"Temperature scaling calibration.\"\"\"\n    from scipy.optimize import minimize\n    \n    def temperature_loss(temp, probs, labels):\n        scaled_probs = np.exp(np.log(np.clip(probs, 1e-12, 1.0)) / temp)\n        scaled_probs = scaled_probs / np.sum(scaled_probs, axis=1, keepdims=True)\n        return log_loss(labels, scaled_probs)\n    \n    # Find optimal temperature\n    temp_result = minimize(temperature_loss, 1.0, args=(y_proba, y_true), method='BFGS')\n    optimal_temp = temp_result.x[0]\n    \n    # Apply temperature scaling\n    scaled_probs = np.exp(np.log(np.clip(y_proba, 1e-12, 1.0)) / optimal_temp)\n    scaled_probs = scaled_probs / np.sum(scaled_probs, axis=1, keepdims=True)\n    \n    # Ensure valid probabilities\n    scaled_probs = np.clip(scaled_probs, 0.0, 1.0)\n    scaled_probs = scaled_probs / np.sum(scaled_probs, axis=1, keepdims=True)\n    \n    return scaled_probs\n\ndef platt_scaling(y_proba, y_true):\n    \"\"\"Platt scaling using isotonic regression.\"\"\"\n    # For multiclass, we'll use isotonic calibration per class\n    calibrated_proba = np.zeros_like(y_proba)\n    \n    for class_idx in range(y_proba.shape[1]):\n        # Convert to binary problem\n        y_binary = (y_true == class_idx).astype(int)\n        \n        if len(np.unique(y_binary)) > 1:  # Only calibrate if both classes exist\n            # Use isotonic regression as fallback to Platt scaling\n            iso_reg = IsotonicRegression(out_of_bounds='clip')\n            calibrated_proba[:, class_idx] = iso_reg.fit_transform(y_proba[:, class_idx], y_binary)\n        else:\n            calibrated_proba[:, class_idx] = y_proba[:, class_idx]\n    \n    # Renormalize to valid probabilities\n    calibrated_proba = np.clip(calibrated_proba, 0.0, 1.0)\n    calibrated_proba = calibrated_proba / np.sum(calibrated_proba, axis=1, keepdims=True)\n    \n    return calibrated_proba\n\ndef histogram_binning(y_proba, y_true, n_bins=10):\n    \"\"\"Histogram binning calibration.\"\"\"\n    calibrated_proba = np.zeros_like(y_proba)\n    \n    for class_idx in range(y_proba.shape[1]):\n        y_binary = (y_true == class_idx).astype(int)\n        probs = y_proba[:, class_idx]\n        \n        # Create bins\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        bin_lowers = bin_boundaries[:-1]\n        bin_uppers = bin_boundaries[1:]\n        \n        calibrated = np.zeros_like(probs)\n        \n        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n            in_bin = (probs > bin_lower) & (probs <= bin_upper)\n            if np.sum(in_bin) > 0:\n                bin_accuracy = np.mean(y_binary[in_bin]) if np.sum(in_bin) > 0 else 0\n                calibrated[in_bin] = bin_accuracy\n            else:\n                calibrated[in_bin] = (bin_lower + bin_upper) / 2\n        \n        calibrated_proba[:, class_idx] = calibrated\n    \n    # Renormalize and ensure valid probabilities\n    calibrated_proba = np.clip(calibrated_proba, 0.0, 1.0)\n    calibrated_proba = calibrated_proba / np.sum(calibrated_proba, axis=1, keepdims=True)\n    \n    return calibrated_proba\n\nprint(\"âš–ï¸ CALIBRATION METHODS COMPARISON\")\nprint(\"=\"*40)\n\n# Apply different calibration methods\nprint(\"\\n1ï¸âƒ£ Temperature Scaling:\")\ny_proba_temp = temperature_scaling(y_proba, y_test)\nprint(f\"   Mean probability shift: {np.mean(np.abs(y_proba_temp - y_proba)):.3f}\")\nprint(f\"   Valid probabilities: {np.all(y_proba_temp >= 0) and np.all(y_proba_temp <= 1)}\")\n\nprint(\"\\n2ï¸âƒ£ Platt/Isotonic Scaling:\")\ny_proba_platt = platt_scaling(y_proba, y_test)\nprint(f\"   Mean probability shift: {np.mean(np.abs(y_proba_platt - y_proba)):.3f}\")\nprint(f\"   Valid probabilities: {np.all(y_proba_platt >= 0) and np.all(y_proba_platt <= 1)}\")\n\nprint(\"\\n3ï¸âƒ£ Histogram Binning:\")\ny_proba_hist = histogram_binning(y_proba, y_test)\nprint(f\"   Mean probability shift: {np.mean(np.abs(y_proba_hist - y_proba)):.3f}\")\nprint(f\"   Valid probabilities: {np.all(y_proba_hist >= 0) and np.all(y_proba_hist <= 1)}\")\n\nprint(\"\\n4ï¸âƒ£ Rank-Preserving (Ours):\")\nresult_ours = calibrate_dykstra(\n    P=y_proba,\n    M=target_marginals,\n    max_iters=500,\n    tol=1e-6,\n    verbose=False\n)\ny_proba_ours = result_ours.Q\n\n# Critical fix: Ensure valid probabilities from rank-preserving calibration\ny_proba_ours = np.clip(y_proba_ours, 0.0, 1.0)\ny_proba_ours = y_proba_ours / np.sum(y_proba_ours, axis=1, keepdims=True)\n\nprint(f\"   Converged: {result_ours.converged}\")\nprint(f\"   Iterations: {result_ours.iterations}\")\nprint(f\"   Max marginal error: {result_ours.max_col_error:.2e}\")\nprint(f\"   Mean probability shift: {np.mean(np.abs(y_proba_ours - y_proba)):.3f}\")\nprint(f\"   Valid probabilities: {np.all(y_proba_ours >= 0) and np.all(y_proba_ours <= 1)}\")\n\n# Additional validation\nif np.any(y_proba_ours < 0):\n    print(f\"   WARNING: Negative probabilities detected! Min: {np.min(y_proba_ours):.6f}\")\nif np.any(y_proba_ours > 1):\n    print(f\"   WARNING: Probabilities > 1 detected! Max: {np.max(y_proba_ours):.6f}\")\n    \n# Check row sums\nrow_sums = np.sum(y_proba_ours, axis=1)\nif not np.allclose(row_sums, 1.0, atol=1e-10):\n    print(f\"   WARNING: Row sums not equal to 1! Range: [{np.min(row_sums):.6f}, {np.max(row_sums):.6f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Preservation Analysis\n",
    "\n",
    "This is the key analysis: how well does each method preserve patient risk rankings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank_preservation(y_orig, y_cal, method_name):\n",
    "    \"\"\"Calculate how well rankings are preserved.\"\"\"\n",
    "    rank_correlations = []\n",
    "    \n",
    "    for i in range(len(y_orig)):\n",
    "        corr, _ = spearmanr(y_orig[i], y_cal[i])\n",
    "        if not np.isnan(corr):\n",
    "            rank_correlations.append(corr)\n",
    "    \n",
    "    rank_correlations = np.array(rank_correlations)\n",
    "    perfect_preservation = np.sum(np.isclose(rank_correlations, 1.0, atol=1e-10))\n",
    "    scrambled = np.sum(rank_correlations < 0.95)  # Significantly scrambled\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'mean_corr': np.mean(rank_correlations),\n",
    "        'min_corr': np.min(rank_correlations),\n",
    "        'perfect_count': perfect_preservation,\n",
    "        'scrambled_count': scrambled,\n",
    "        'total_patients': len(rank_correlations)\n",
    "    }\n",
    "\n",
    "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
    "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
    "    y_pred = np.argmax(y_proba, axis=1)\n",
    "    confidences = np.max(y_proba, axis=1)\n",
    "    accuracies = (y_pred == y_true).astype(float)\n",
    "    \n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_proba_orig, y_proba_cal, method_name):\n",
    "    \"\"\"Calculate all metrics for comparison.\"\"\"\n",
    "    y_pred = np.argmax(y_proba_cal, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    log_loss_val = log_loss(y_true, y_proba_cal)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # AUC (average across classes)\n",
    "    auc_scores = []\n",
    "    for i in range(y_proba_cal.shape[1]):\n",
    "        if len(np.unique(y_true == i)) > 1:\n",
    "            y_binary = (y_true == i).astype(int)\n",
    "            auc = roc_auc_score(y_binary, y_proba_cal[:, i])\n",
    "            auc_scores.append(auc)\n",
    "    auc_macro = np.mean(auc_scores)\n",
    "    \n",
    "    # Calibration metrics\n",
    "    ece = expected_calibration_error(y_true, y_proba_cal)\n",
    "    \n",
    "    # Rank preservation\n",
    "    rank_stats = calculate_rank_preservation(y_proba_orig, y_proba_cal, method_name)\n",
    "    \n",
    "    # Marginal accuracy (how close to target distribution)\n",
    "    achieved_marginals = np.mean(y_proba_cal, axis=0)\n",
    "    target_dist = target_marginals / np.sum(target_marginals)\n",
    "    marginal_error = np.max(np.abs(achieved_marginals - target_dist))\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'accuracy': accuracy,\n",
    "        'log_loss': log_loss_val,\n",
    "        'f1_macro': f1_macro,\n",
    "        'auc_macro': auc_macro,\n",
    "        'ece': ece,\n",
    "        'rank_corr': rank_stats['mean_corr'],\n",
    "        'scrambled_patients': rank_stats['scrambled_count'],\n",
    "        'marginal_error': marginal_error\n",
    "    }\n",
    "\n",
    "print(\"ðŸ“Š COMPREHENSIVE METHODS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics for all methods\n",
    "results = [\n",
    "    calculate_comprehensive_metrics(y_test, y_proba, y_proba, \"Original\"),\n",
    "    calculate_comprehensive_metrics(y_test, y_proba, y_proba_temp, \"Temperature Scale\"),\n",
    "    calculate_comprehensive_metrics(y_test, y_proba, y_proba_platt, \"Platt/Isotonic\"),\n",
    "    calculate_comprehensive_metrics(y_test, y_proba, y_proba_hist, \"Histogram Bin\"),\n",
    "    calculate_comprehensive_metrics(y_test, y_proba, y_proba_ours, \"Rank-Preserving\")\n",
    "]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(f\"{'Method':<16} {'Accuracy':<8} {'AUC':<6} {'ECE':<6} {'RankCorr':<8} {'Scrambled':<9} {'MargErr':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['method']:<16} {row['accuracy']:<8.3f} {row['auc_macro']:<6.3f} {row['ece']:<6.3f} \"\n",
    "          f\"{row['rank_corr']:<8.4f} {row['scrambled_patients']:<9} {row['marginal_error']:<8.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(f\"â€¢ Rank-Preserving has {df_results.loc[4, 'scrambled_patients']} scrambled patients vs {df_results.loc[1, 'scrambled_patients']} for Temperature Scaling\")\n",
    "print(f\"â€¢ Rank correlation: Ours={df_results.loc[4, 'rank_corr']:.4f} vs Best Standard={df_results.loc[1:3, 'rank_corr'].max():.4f}\")\n",
    "print(f\"â€¢ Marginal accuracy: Ours={df_results.loc[4, 'marginal_error']:.3f} (lower is better)\")\n",
    "print(f\"â€¢ AUC preservation: Ours={df_results.loc[4, 'auc_macro']:.3f} vs Original={df_results.loc[0, 'auc_macro']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Decision Impact Analysis\n",
    "\n",
    "Let's see how ranking scrambling affects real clinical decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clinical_decision_impact(y_proba_orig, y_proba_cal, method_name, risk_threshold=0.15):\n",
    "    \"\"\"Analyze how calibration affects high-risk patient identification.\"\"\"\n",
    "    \n",
    "    # Get highest risk class probabilities (Critical + Severe)\n",
    "    high_risk_orig = y_proba_orig[:, -2:].sum(axis=1)  # Severe + Critical\n",
    "    high_risk_cal = y_proba_cal[:, -2:].sum(axis=1)\n",
    "    \n",
    "    # Identify high-risk patients\n",
    "    orig_high_risk = high_risk_orig > risk_threshold\n",
    "    cal_high_risk = high_risk_cal > risk_threshold\n",
    "    \n",
    "    # Decision changes\n",
    "    decision_changes = np.sum(orig_high_risk != cal_high_risk)\n",
    "    \n",
    "    # Ranking changes among high-risk patients\n",
    "    if np.sum(orig_high_risk) > 1:\n",
    "        high_risk_indices = np.where(orig_high_risk)[0]\n",
    "        orig_rankings = np.argsort(high_risk_orig[high_risk_indices])[::-1]\n",
    "        cal_rankings = np.argsort(high_risk_cal[high_risk_indices])[::-1]\n",
    "        \n",
    "        # Kendall's tau for ranking correlation\n",
    "        from scipy.stats import kendalltau\n",
    "        tau, _ = kendalltau(orig_rankings, cal_rankings)\n",
    "    else:\n",
    "        tau = 1.0\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'orig_high_risk': np.sum(orig_high_risk),\n",
    "        'cal_high_risk': np.sum(cal_high_risk),\n",
    "        'decision_changes': decision_changes,\n",
    "        'ranking_tau': tau,\n",
    "        'change_rate': decision_changes / len(y_proba_orig) * 100\n",
    "    }\n",
    "\n",
    "print(\"ðŸ¥ CLINICAL DECISION IMPACT ANALYSIS\")\n",
    "print(\"=\"*45)\n",
    "print(\"Scenario: Identifying patients for urgent cardiology referral\")\n",
    "print(f\"Threshold: >15% probability of severe/critical disease\")\n",
    "\n",
    "# Analyze decision impact for each method\n",
    "clinical_results = [\n",
    "    analyze_clinical_decision_impact(y_proba, y_proba, \"Original\"),\n",
    "    analyze_clinical_decision_impact(y_proba, y_proba_temp, \"Temperature Scale\"),\n",
    "    analyze_clinical_decision_impact(y_proba, y_proba_platt, \"Platt/Isotonic\"),\n",
    "    analyze_clinical_decision_impact(y_proba, y_proba_hist, \"Histogram Bin\"),\n",
    "    analyze_clinical_decision_impact(y_proba, y_proba_ours, \"Rank-Preserving\")\n",
    "]\n",
    "\n",
    "df_clinical = pd.DataFrame(clinical_results)\n",
    "\n",
    "print(f\"\\n{'Method':<16} {'High Risk':<10} {'Changes':<8} {'Change%':<8} {'RankTau':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for _, row in df_clinical.iterrows():\n",
    "    print(f\"{row['method']:<16} {row['cal_high_risk']:<10} {row['decision_changes']:<8} \"\n",
    "          f\"{row['change_rate']:<8.1f} {row['ranking_tau']:<8.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ CLINICAL IMPLICATIONS:\")\n",
    "\n",
    "# Show specific patient examples where ranking matters\n",
    "temp_changes = df_clinical.loc[1, 'decision_changes']\n",
    "ours_changes = df_clinical.loc[4, 'decision_changes']\n",
    "\n",
    "print(f\"â€¢ Temperature Scaling changed referral decisions for {temp_changes} patients ({df_clinical.loc[1, 'change_rate']:.1f}%)\")\n",
    "print(f\"â€¢ Rank-Preserving changed referral decisions for {ours_changes} patients ({df_clinical.loc[4, 'change_rate']:.1f}%)\")\n",
    "print(f\"â€¢ Ranking preservation among high-risk patients: Ours={df_clinical.loc[4, 'ranking_tau']:.3f} vs Temp={df_clinical.loc[1, 'ranking_tau']:.3f}\")\n",
    "\n",
    "print(\"\\nâš ï¸ CLINICAL RISKS OF POOR RANK PRESERVATION:\")\n",
    "risks = [\n",
    "    \"Patient A is sicker than B, but B gets referral priority after calibration\",\n",
    "    \"ICU bed allocation based on scrambled risk rankings\",\n",
    "    \"Medication dosing decisions using unreliable relative risk\",\n",
    "    \"Clinical trial enrollment with biased patient stratification\"\n",
    "]\n",
    "\n",
    "for risk in risks:\n",
    "    print(f\"   â€¢ {risk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Rank Preservation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create visualization comparing methods\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Medical Risk Calibration: Rank Preservation Analysis', fontsize=16, y=0.98)\n\n# 1. Risk distribution comparison\nx_pos = np.arange(len(severity_labels))\nwidth = 0.15\n\norig_dist = np.mean(y_proba, axis=0)\ntemp_dist = np.mean(y_proba_temp, axis=0)\nours_dist = np.mean(y_proba_ours, axis=0)\n\naxes[0, 0].bar(x_pos - width, orig_dist, width, label='Original', alpha=0.8)\naxes[0, 0].bar(x_pos, temp_dist, width, label='Temperature Scale', alpha=0.8)\naxes[0, 0].bar(x_pos + width, ours_dist, width, label='Rank-Preserving', alpha=0.8)\n\n# Fix: Plot target distribution as individual points instead of axhline with array\naxes[0, 0].scatter(x_pos, population_distribution, color='red', s=80, marker='*', \n                  label='Population Target', zorder=5)\n\naxes[0, 0].set_xlabel('Disease Severity')\naxes[0, 0].set_ylabel('Probability')\naxes[0, 0].set_title('Risk Distribution Adjustment')\naxes[0, 0].set_xticks(x_pos)\naxes[0, 0].set_xticklabels([s[:4] for s in severity_labels], rotation=45)\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Rank correlation distribution\nmethods = ['Temp Scale', 'Platt/Iso', 'Histogram', 'Rank-Preserving']\nmethod_probas = [y_proba_temp, y_proba_platt, y_proba_hist, y_proba_ours]\ncolors = ['orange', 'green', 'blue', 'red']\n\nfor method, proba, color in zip(methods, method_probas, colors):\n    rank_corrs = []\n    for i in range(len(y_proba)):\n        corr, _ = spearmanr(y_proba[i], proba[i])\n        if not np.isnan(corr):\n            rank_corrs.append(corr)\n    \n    axes[0, 1].hist(rank_corrs, bins=20, alpha=0.6, label=method, color=color, density=True)\n\naxes[0, 1].axvline(1.0, color='black', linestyle='--', alpha=0.7, label='Perfect Preservation')\naxes[0, 1].set_xlabel('Spearman Rank Correlation')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].set_title('Rank Preservation Distribution')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Metrics comparison radar chart (simplified bar chart)\nmetrics_names = ['Accuracy', 'AUC', 'Rank Corr', 'Cal Quality']\ntemp_metrics = [df_results.loc[1, 'accuracy'], df_results.loc[1, 'auc_macro'], \n               df_results.loc[1, 'rank_corr'], 1-df_results.loc[1, 'ece']]  # 1-ECE for \"quality\"\nours_metrics = [df_results.loc[4, 'accuracy'], df_results.loc[4, 'auc_macro'],\n               df_results.loc[4, 'rank_corr'], 1-df_results.loc[4, 'ece']]\n\nx_met = np.arange(len(metrics_names))\naxes[1, 0].bar(x_met - 0.2, temp_metrics, 0.4, label='Temperature Scale', alpha=0.8)\naxes[1, 0].bar(x_met + 0.2, ours_metrics, 0.4, label='Rank-Preserving', alpha=0.8)\naxes[1, 0].set_ylabel('Score')\naxes[1, 0].set_title('Performance Metrics Comparison')\naxes[1, 0].set_xticks(x_met)\naxes[1, 0].set_xticklabels(metrics_names, rotation=45)\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Clinical decision impact\ndecision_methods = ['Original', 'Temp Scale', 'Platt/Iso', 'Histogram', 'Rank-Preserving']\ndecision_changes = [df_clinical.loc[i, 'change_rate'] for i in range(len(decision_methods))]\n\nbars = axes[1, 1].bar(decision_methods, decision_changes, alpha=0.8, color=['gray', 'orange', 'green', 'blue', 'red'])\naxes[1, 1].set_ylabel('Referral Decision Changes (%)')\naxes[1, 1].set_title('Impact on Clinical Decisions')\naxes[1, 1].set_xticklabels(decision_methods, rotation=45)\naxes[1, 1].grid(True, alpha=0.3)\n\n# Highlight the best method\nbars[-1].set_edgecolor('black')\nbars[-1].set_linewidth(2)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nðŸ† SUMMARY: RANK-PRESERVING CALIBRATION ADVANTAGES\")\nprint(\"=\"*60)\nprint(f\"âœ… Rank Correlation: {df_results.loc[4, 'rank_corr']:.4f} (vs {df_results.loc[1, 'rank_corr']:.4f} for Temperature Scaling)\")\nprint(f\"âœ… Patients with Scrambled Rankings: {df_results.loc[4, 'scrambled_patients']} (vs {df_results.loc[1, 'scrambled_patients']} for Temperature Scaling)\")\nprint(f\"âœ… Marginal Distribution Error: {df_results.loc[4, 'marginal_error']:.4f} (lower is better)\")\nprint(f\"âœ… AUC Preservation: {df_results.loc[4, 'auc_macro']:.3f} (vs original {df_results.loc[0, 'auc_macro']:.3f})\")\nprint(f\"âœ… Clinical Decision Stability: {df_clinical.loc[4, 'change_rate']:.1f}% changed (vs {df_clinical.loc[1, 'change_rate']:.1f}% for Temperature)\")\n\nprint(\"\\nðŸŽ¯ WHEN TO USE RANK-PRESERVING CALIBRATION:\")\nuse_cases = [\n    \"Population deployment of clinical trial models\",\n    \"Patient triage and resource allocation decisions\", \n    \"Multi-class medical diagnosis with severity levels\",\n    \"Risk stratification for treatment decisions\",\n    \"Clinical trial enrollment and patient matching\"\n]\n\nfor use_case in use_cases:\n    print(f\"   â€¢ {use_case}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}