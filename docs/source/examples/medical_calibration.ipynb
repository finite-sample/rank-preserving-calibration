{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Medical Survey Research: Population Health Calibration\n\n**Core Problem**: Survey statisticians and machine learning practitioners often need to adjust predicted class probabilities from a classifier so they match known population totals (column marginals). Simple post-hoc methods that apply separate logit shifts or raking to each class can scramble the ranking of individuals within a class when there are three or more classes.\n\nThis example demonstrates this problem using the Wisconsin Breast Cancer dataset, simulating a health survey scenario where:\n1. A diagnostic model is trained on hospital data (biased sample)\n2. We need to calibrate predictions to match true population health statistics\n3. **Critical requirement**: Preserve individual patient risk rankings while adjusting marginals\n\n## Medical Survey Context\n\n**Scenario**: Large-scale health screening survey where:\n- **Sampling bias**: Hospital training data over-represents high-risk patients  \n- **Population matching**: Need to match Census health demographics\n- **Multiple risk categories**: Low, Medium, High risk (3+ classes)\n- **Ranking preservation**: Individual likelihood orderings must be maintained"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import brier_score_loss, roc_auc_score\nfrom scipy.stats import spearmanr\nimport pandas as pd\n\n# Import our calibration package - proper imports (no sys.path hacks!)\nfrom rank_preserving_calibration import (\n    calibrate_dykstra, calibrate_admm,  # Two algorithms\n    feasibility_metrics, isotonic_metrics, distance_metrics,  # Rich metrics\n    sharpness_metrics, classwise_ece  # Advanced metrics\n)\n\n# Set style for publication-quality plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n\nprint(\"üè• MEDICAL SURVEY CALIBRATION TOOLKIT LOADED\")\nprint(\"Focus: Rank-preserving multinomial calibration for health surveys\")\nprint(\"Package features: Dykstra, ADMM, nearly-isotonic, comprehensive metrics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Health Survey Data with Multinomial Risk Categories\n\nWe'll create a realistic health survey scenario with 3 risk categories (Low, Medium, High) to demonstrate the core multinomial calibration problem."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üè• CREATING BIASED HEALTH SURVEY DATA\")\nprint(\"=\"*50)\n\n# Load breast cancer data and create multinomial risk categories  \ndata = load_breast_cancer()\nX, y_binary = data.data, data.target\n\n# Convert to multinomial: Create 3 risk categories based on features\n# This simulates a health survey where we classify patients into risk levels\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Create synthetic 3-class problem: Low, Medium, High risk\n# Use feature combinations to create realistic risk stratification\nrisk_score = (\n    X_scaled[:, 0] * 0.3 +  # mean radius\n    X_scaled[:, 7] * 0.3 +  # mean concavity  \n    X_scaled[:, 20] * 0.2 + # worst radius\n    X_scaled[:, 27] * 0.2   # worst concavity\n)\n\n# Convert to 3-class labels based on percentiles (hospital bias)\n# Hospital data biased toward high-risk patients\npercentiles = [40, 75]  # Biased split: more high-risk\ny_multinomial = np.digitize(risk_score, np.percentile(risk_score, percentiles))\n\n# Create labels\nrisk_labels = ['Low Risk', 'Medium Risk', 'High Risk']\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Risk categories: {risk_labels}\")\n\n# Split data - hospital training sample  \nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y_multinomial, test_size=0.3, random_state=42, stratify=y_multinomial\n)\n\n# Show hospital sample bias\nhospital_distribution = np.bincount(y_train) / len(y_train)\nprint(f\"\\nHOSPITAL TRAINING SAMPLE (biased):\")\nfor i, (label, pct) in enumerate(zip(risk_labels, hospital_distribution)):\n    print(f\"  {label}: {np.sum(y_train == i):,} ({pct:.1%})\")\n\nprint(f\"\\nTest sample size: {len(y_test)}\")\nprint(f\"Features: {X.shape[1]} clinical measurements\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Training and Survey Bias"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train multinomial classifier on biased hospital data\nprint(\"ü§ñ TRAINING HEALTH RISK CLASSIFIER\")\nprint(\"=\"*40)\n\n# Train Random Forest for 3-class classification\nmodel = RandomForestClassifier(\n    n_estimators=100, \n    max_depth=10, \n    random_state=42,\n    class_weight='balanced'\n)\nmodel.fit(X_train, y_train)\n\n# Get predictions and probabilities on test set\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)  # Shape: (n_test, 3)\n\n# Model performance\nfrom sklearn.metrics import accuracy_score, classification_report\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Classifier accuracy: {accuracy:.3f}\")\nprint(f\"Test samples: {len(y_test)}\")\n\n# Current model marginals (reflects hospital bias)\ncurrent_marginals = np.mean(y_proba, axis=0)\nprint(f\"\\nMODEL PREDICTED DISTRIBUTION (hospital bias):\")\nfor i, (label, marginal) in enumerate(zip(risk_labels, current_marginals)):\n    print(f\"  {label}: {marginal:.3f} ({marginal*100:.1f}%)\")\n\n# Show feature importance\nfeature_names = ['radius', 'texture', 'perimeter', 'area', 'smoothness'][:5]\nimportance = model.feature_importances_[:5]\nprint(f\"\\nTop predictive features:\")\nfor feat, imp in zip(feature_names, importance):\n    print(f\"  {feat}: {imp:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Population Health Statistics and Calibration Challenge\n\n**Key Problem**: Our hospital-trained model must be calibrated to match true population health demographics from Census/health survey data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìä POPULATION HEALTH TARGETS\")\nprint(\"=\"*40)\n\n# True population health distribution (from Census/health surveys)\n# General population has different risk distribution than hospital sample\npopulation_health_distribution = np.array([\n    0.60,   # Low Risk: Higher in general population\n    0.30,   # Medium Risk: Moderate \n    0.10    # High Risk: Much lower than hospital sample\n])\n\nprint(\"TARGET POPULATION HEALTH DISTRIBUTION (Census data):\")\nfor i, (label, target_pct) in enumerate(zip(risk_labels, population_health_distribution)):\n    current_pct = current_marginals[i]\n    change = target_pct - current_pct\n    direction = \"‚Üë\" if change > 0 else \"‚Üì\" if change < 0 else \"‚Üí\"\n    print(f\"  {label}: {target_pct:.1%} (change: {change:+.1%} {direction})\")\n\nprint(f\"\\nüéØ MULTINOMIAL CALIBRATION CHALLENGE:\")\nprint(f\"   ‚Ä¢ Hospital model biased toward high-risk patients\")\nprint(f\"   ‚Ä¢ Need to match Census population health marginals\")  \nprint(f\"   ‚Ä¢ Must preserve individual patient risk rankings\")\nprint(f\"   ‚Ä¢ Simple logit shifts will scramble patient orderings\")\n\n# Calculate target marginals for calibration\nn_test_samples = len(y_test)\ntarget_marginals = population_health_distribution * n_test_samples\n\nprint(f\"\\nCALIBRATION PARAMETERS:\")\nprint(f\"  Test samples: {n_test_samples}\")\nprint(f\"  Target marginals: {target_marginals}\")\nprint(f\"  Sum check: {np.sum(target_marginals):.1f} (should equal {n_test_samples})\")\n\nprint(f\"\\nüö® WHY RANKING PRESERVATION IS CRITICAL:\")\nranking_importance = [\n    \"Individual patient triage depends on relative risk rankings\",\n    \"Resource allocation requires preserved within-category orderings\", \n    \"Treatment priority decisions based on individual likelihood rankings\",\n    \"Health economics models depend on maintained patient risk orderings\"\n]\n\nfor importance in ranking_importance:\n    print(f\"   ‚Ä¢ {importance}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demonstrating Simple Methods Fail (Ranking Scrambling)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"‚ö†Ô∏è DEMONSTRATING RANKING SCRAMBLING WITH SIMPLE METHODS\")\nprint(\"=\"*65)\n\n# Simple post-hoc method: separate logit shifts per class\ndef simple_logit_calibration(probs, targets):\n    \"\"\"Apply separate logit shifts - CAN SCRAMBLE RANKINGS with 3+ classes\"\"\"\n    current_marginals = np.mean(probs, axis=0)\n    \n    # Calculate logit shifts for each risk category\n    logit_shifts = np.log(targets / np.sum(targets)) - np.log(current_marginals)\n    \n    # Apply shifts\n    log_probs = np.log(probs + 1e-12)\n    shifted_log_probs = log_probs + logit_shifts[np.newaxis, :]\n    \n    # Renormalize\n    shifted_probs = np.exp(shifted_log_probs)\n    calibrated_probs = shifted_probs / np.sum(shifted_probs, axis=1, keepdims=True)\n    \n    return calibrated_probs\n\n# Apply simple method\ny_proba_simple = simple_logit_calibration(y_proba, population_health_distribution)\n\n# Check ranking preservation with simple method\nprint(\"RANKING PRESERVATION ANALYSIS - SIMPLE LOGIT METHOD:\")\nsimple_rank_correlations = []\nfor i in range(len(y_test)):\n    corr, _ = spearmanr(y_proba[i], y_proba_simple[i])\n    if not np.isnan(corr):\n        simple_rank_correlations.append(corr)\n\nsimple_rank_correlations = np.array(simple_rank_correlations)\nperfect_simple = np.sum(np.isclose(simple_rank_correlations, 1.0, atol=1e-10))\nscrambled_simple = np.sum(simple_rank_correlations < 0.95)\n\nprint(f\"  Perfect rank preservation: {perfect_simple}/{len(simple_rank_correlations)}\")\nprint(f\"  Significantly scrambled (corr < 0.95): {scrambled_simple}\")\nprint(f\"  Mean Spearman correlation: {np.mean(simple_rank_correlations):.3f}\")\n\n# Check marginal accuracy\nsimple_achieved = np.mean(y_proba_simple, axis=0)\nsimple_marginal_error = np.max(np.abs(simple_achieved - population_health_distribution))\nprint(f\"  Maximum marginal error: {simple_marginal_error:.4f}\")\n\nprint(f\"\\n‚ùå PROBLEMS WITH SIMPLE LOGIT METHOD:\")\nprint(f\"   ‚Ä¢ Scrambles rankings in {scrambled_simple} patient cases\")\nprint(f\"   ‚Ä¢ Patient A more likely High Risk than B before calibration\")\nprint(f\"   ‚Ä¢ But less likely High Risk than B after calibration\")\nprint(f\"   ‚Ä¢ Violates clinical triage and priority principles\")\nprint(f\"   ‚Ä¢ Makes individual patient risk assessment unreliable\")\n\n# Show examples of ranking violations\nif scrambled_simple > 0:\n    worst_cases = np.argsort(simple_rank_correlations)[:3]\n    print(f\"\\nüîç EXAMPLES OF RANKING SCRAMBLING:\")\n    for i, case_idx in enumerate(worst_cases):\n        orig_order = np.argsort(-y_proba[case_idx])\n        simple_order = np.argsort(-y_proba_simple[case_idx])\n        corr = simple_rank_correlations[case_idx]\n        \n        orig_labels = [risk_labels[j] for j in orig_order]\n        simple_labels = [risk_labels[j] for j in simple_order]\n        \n        print(f\"  Patient {case_idx}: Correlation = {corr:.3f}\")\n        print(f\"    Original ranking: {' > '.join(orig_labels)}\")\n        print(f\"    Simple method:   {' > '.join(simple_labels)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Algorithm Comparison: Dykstra vs ADMM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üî¨ ALGORITHM COMPARISON: DYKSTRA vs ADMM\")\nprint(\"=\"*50)\n\n# Method 1: Dykstra's alternating projections (recommended default)\nprint(\"1Ô∏è‚É£ DYKSTRA'S ALTERNATING PROJECTIONS:\")\nresult_dykstra = calibrate_dykstra(\n    P=y_proba,\n    M=target_marginals,\n    max_iters=2000,\n    tol=1e-7,\n    verbose=True\n)\n\ny_proba_dykstra = result_dykstra.Q\nprint(f\"\\n   Converged: {result_dykstra.converged}\")\nprint(f\"   Iterations: {result_dykstra.iterations}\")\nprint(f\"   Final objective: {result_dykstra.objective:.2e}\")\nprint(f\"   Max column error: {result_dykstra.max_col_error:.2e}\")\nprint(f\"   Max rank violation: {result_dykstra.max_rank_violation:.2e}\")\n\n# Method 2: ADMM optimization (alternative with convergence history)\nprint(f\"\\n2Ô∏è‚É£ ADMM OPTIMIZATION:\")\nresult_admm = calibrate_admm(\n    P=y_proba,\n    M=target_marginals,\n    max_iters=1000,\n    tol=1e-6,\n    verbose=True\n)\n\ny_proba_admm = result_admm.Q\nprint(f\"\\n   Converged: {result_admm.converged}\")\nprint(f\"   Iterations: {result_admm.iterations}\")\nprint(f\"   Final objective: {result_admm.objective:.2e}\")\nprint(f\"   Max column error: {result_admm.max_col_error:.2e}\")\nprint(f\"   Max rank violation: {result_admm.max_rank_violation:.2e}\")\n\n# Comparison of results\nprint(f\"\\nüìä ALGORITHM COMPARISON:\")\nprint(f\"{'Metric':<20} {'Dykstra':<15} {'ADMM':<15}\")\nprint(\"-\" * 50)\nprint(f\"{'Converged':<20} {result_dykstra.converged:<15} {result_admm.converged:<15}\")\nprint(f\"{'Iterations':<20} {result_dykstra.iterations:<15} {result_admm.iterations:<15}\")\nprint(f\"{'Final objective':<20} {result_dykstra.objective:<15.2e} {result_admm.objective:<15.2e}\")\nprint(f\"{'Max col error':<20} {result_dykstra.max_col_error:<15.2e} {result_admm.max_col_error:<15.2e}\")\n\n# Check if both methods give same result\nprob_difference = np.max(np.abs(y_proba_dykstra - y_proba_admm))\nprint(f\"\\nMaximum probability difference: {prob_difference:.2e}\")\nprint(f\"Methods agree: {'Yes' if prob_difference < 1e-6 else 'No'}\")\n\nprint(f\"\\nüéØ WHEN TO USE EACH ALGORITHM:\")\nprint(f\"   ‚Ä¢ Dykstra: Default choice, exact projections, reliable convergence\")\nprint(f\"   ‚Ä¢ ADMM: When you need convergence diagnostics, experimental features\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Nearly Isotonic Constraints (Relaxed Rank Preservation)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üîÑ NEARLY ISOTONIC CALIBRATION (RELAXED CONSTRAINTS)\")  \nprint(\"=\"*60)\n\nprint(\"Sometimes strict rank preservation is too restrictive...\")\nprint(\"Nearly-isotonic allows small ranking violations for better fit\")\n\n# Method 1: Epsilon-slack approach (Dykstra)\nprint(\"\\n1Ô∏è‚É£ EPSILON-SLACK APPROACH:\")\nprint(\"   Allows z[i+1] >= z[i] - eps instead of strict z[i+1] >= z[i]\")\n\nnearly_epsilon = {\"mode\": \"epsilon\", \"eps\": 0.05}\nresult_nearly_eps = calibrate_dykstra(\n    P=y_proba,\n    M=target_marginals,\n    nearly=nearly_epsilon,\n    max_iters=2000,\n    tol=1e-7,\n    verbose=True\n)\n\ny_proba_nearly_eps = result_nearly_eps.Q\nprint(f\"\\n   Converged: {result_nearly_eps.converged}\")\nprint(f\"   Iterations: {result_nearly_eps.iterations}\")\nprint(f\"   Max rank violation: {result_nearly_eps.max_rank_violation:.2e}\")\n\n# Method 2: Lambda-penalty approach (ADMM)\nprint(\"\\n2Ô∏è‚É£ LAMBDA-PENALTY APPROACH (Experimental):\")\nprint(\"   Penalizes ranking violations with Œª * sum(violations)\")\n\nnearly_lambda = {\"mode\": \"lambda\", \"lam\": 1.0}\nresult_nearly_lam = calibrate_admm(\n    P=y_proba,\n    M=target_marginals,\n    nearly=nearly_lambda,\n    max_iters=1000,\n    tol=1e-6,\n    verbose=True\n)\n\ny_proba_nearly_lam = result_nearly_lam.Q\nprint(f\"\\n   Converged: {result_nearly_lam.converged}\")\nprint(f\"   Iterations: {result_nearly_lam.iterations}\")\nprint(f\"   Max rank violation: {result_nearly_lam.max_rank_violation:.2e}\")\n\n# Compare ranking preservation\ndef check_ranking_preservation(P_orig, P_cal, method_name):\n    \"\"\"Check how well rankings are preserved\"\"\"\n    rank_correlations = []\n    for i in range(len(P_orig)):\n        corr, _ = spearmanr(P_orig[i], P_cal[i])\n        if not np.isnan(corr):\n            rank_correlations.append(corr)\n    \n    rank_correlations = np.array(rank_correlations)\n    perfect = np.sum(np.isclose(rank_correlations, 1.0, atol=1e-10))\n    mean_corr = np.mean(rank_correlations)\n    min_corr = np.min(rank_correlations)\n    \n    print(f\"\\n{method_name} ranking preservation:\")\n    print(f\"   Perfect preservation: {perfect}/{len(rank_correlations)}\")\n    print(f\"   Mean correlation: {mean_corr:.6f}\")\n    print(f\"   Min correlation: {min_corr:.6f}\")\n    \n    return mean_corr\n\n# Check all methods\nstrict_corr = check_ranking_preservation(y_proba, y_proba_dykstra, \"Strict isotonic\")\neps_corr = check_ranking_preservation(y_proba, y_proba_nearly_eps, \"Epsilon-slack\")\nlam_corr = check_ranking_preservation(y_proba, y_proba_nearly_lam, \"Lambda-penalty\")\n\nprint(f\"\\nüéØ WHEN TO USE NEARLY-ISOTONIC:\")\nprint(f\"   ‚Ä¢ Strict isotonic too restrictive for your data\")\nprint(f\"   ‚Ä¢ Small ranking violations acceptable for better marginal fit\")  \nprint(f\"   ‚Ä¢ Epsilon-slack: Maintains convexity, theoretical guarantees\")\nprint(f\"   ‚Ä¢ Lambda-penalty: Experimental, may need parameter tuning\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Comparison\n",
    "\n",
    "Let's quantify the improvement in calibration while showing that discrimination is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various performance metrics\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "\n",
    "# Discrimination metrics (should be unchanged)\n",
    "auc_original = roc_auc_score(y_test, malignant_probs_original)\n",
    "auc_calibrated = roc_auc_score(y_test, malignant_probs_calibrated)\n",
    "\n",
    "# Calibration metrics\n",
    "brier_original = brier_score_loss(y_test, malignant_probs_original)\n",
    "brier_calibrated = brier_score_loss(y_test, malignant_probs_calibrated)\n",
    "\n",
    "logloss_original = log_loss(y_test, malignant_probs_original)\n",
    "logloss_calibrated = log_loss(y_test, malignant_probs_calibrated)\n",
    "\n",
    "# Calibration error (Expected Calibration Error)\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "ece_original = expected_calibration_error(y_test, malignant_probs_original)\n",
    "ece_calibrated = expected_calibration_error(y_test, malignant_probs_calibrated)\n",
    "\n",
    "# Create results summary\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['AUC-ROC', 'Brier Score', 'Log Loss', 'ECE', 'Mean Prediction'],\n",
    "    'Original': [auc_original, brier_original, logloss_original, ece_original, malignant_probs_original.mean()],\n",
    "    'Calibrated': [auc_calibrated, brier_calibrated, logloss_calibrated, ece_calibrated, malignant_probs_calibrated.mean()],\n",
    "    'Target': ['-', '-', '-', 0.0, target_prevalence]\n",
    "})\n",
    "\n",
    "# Calculate improvements\n",
    "results_df['Change'] = results_df['Calibrated'] - results_df['Original']\n",
    "\n",
    "print(\"Performance Metrics Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    if row['Change'] != row['Change']:  # NaN check\n",
    "        print(f\"{row['Metric']:<15} {row['Original']:<12.4f} {row['Calibrated']:<12.4f} {row['Target']:<12}\")\n",
    "    else:\n",
    "        print(f\"{row['Metric']:<15} {row['Original']:<12.4f} {row['Calibrated']:<12.4f} {row['Target']:<12} ({row['Change']:+.4f})\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"‚Ä¢ AUC-ROC maintained: {abs(auc_calibrated - auc_original) < 0.001} (Œî={auc_calibrated-auc_original:.6f})\")\n",
    "print(f\"‚Ä¢ Calibration error reduced: {ece_original:.4f} ‚Üí {ece_calibrated:.4f}\")\n",
    "print(f\"‚Ä¢ Mean prediction corrected: {malignant_probs_original.mean():.3f} ‚Üí {malignant_probs_calibrated.mean():.3f} (target: {target_prevalence:.3f})\")\n",
    "print(f\"‚Ä¢ Brier score {'improved' if brier_calibrated < brier_original else 'changed'}: {brier_original:.4f} ‚Üí {brier_calibrated:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical Decision Analysis\n",
    "\n",
    "Let's analyze how calibration affects clinical decision making at different risk thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical decision analysis\n",
    "def analyze_clinical_decisions(y_true, y_prob_orig, y_prob_cal, thresholds):\n",
    "    \"\"\"Analyze clinical decisions at different thresholds.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        # Original model decisions\n",
    "        decisions_orig = y_prob_orig >= thresh\n",
    "        tp_orig = np.sum((decisions_orig == 1) & (y_true == 1))\n",
    "        fp_orig = np.sum((decisions_orig == 1) & (y_true == 0))\n",
    "        tn_orig = np.sum((decisions_orig == 0) & (y_true == 0))\n",
    "        fn_orig = np.sum((decisions_orig == 0) & (y_true == 1))\n",
    "        \n",
    "        # Calibrated model decisions\n",
    "        decisions_cal = y_prob_cal >= thresh\n",
    "        tp_cal = np.sum((decisions_cal == 1) & (y_true == 1))\n",
    "        fp_cal = np.sum((decisions_cal == 1) & (y_true == 0))\n",
    "        tn_cal = np.sum((decisions_cal == 0) & (y_true == 0))\n",
    "        fn_cal = np.sum((decisions_cal == 0) & (y_true == 1))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sens_orig = tp_orig / (tp_orig + fn_orig) if (tp_orig + fn_orig) > 0 else 0\n",
    "        spec_orig = tn_orig / (tn_orig + fp_orig) if (tn_orig + fp_orig) > 0 else 0\n",
    "        ppv_orig = tp_orig / (tp_orig + fp_orig) if (tp_orig + fp_orig) > 0 else 0\n",
    "        \n",
    "        sens_cal = tp_cal / (tp_cal + fn_cal) if (tp_cal + fn_cal) > 0 else 0\n",
    "        spec_cal = tn_cal / (tn_cal + fp_cal) if (tn_cal + fp_cal) > 0 else 0\n",
    "        ppv_cal = tp_cal / (tp_cal + fp_cal) if (tp_cal + fp_cal) > 0 else 0\n",
    "        \n",
    "        # Decision changes\n",
    "        decision_changes = np.sum(decisions_orig != decisions_cal)\n",
    "        \n",
    "        results.append({\n",
    "            'Threshold': thresh,\n",
    "            'Sensitivity_Orig': sens_orig,\n",
    "            'Sensitivity_Cal': sens_cal,\n",
    "            'Specificity_Orig': spec_orig,\n",
    "            'Specificity_Cal': spec_cal,\n",
    "            'PPV_Orig': ppv_orig,\n",
    "            'PPV_Cal': ppv_cal,\n",
    "            'Decision_Changes': decision_changes,\n",
    "            'Patients_Flagged_Orig': np.sum(decisions_orig),\n",
    "            'Patients_Flagged_Cal': np.sum(decisions_cal)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze at key clinical thresholds\n",
    "clinical_thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "decision_analysis = analyze_clinical_decisions(\n",
    "    y_test, malignant_probs_original, malignant_probs_calibrated, clinical_thresholds\n",
    ")\n",
    "\n",
    "print(\"Clinical Decision Analysis:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Threshold':<10} {'Sensitivity':<20} {'Specificity':<20} {'PPV':<20} {'Changes':<10}\")\n",
    "print(f\"{'':10} {'Orig':>8} {'Cal':>8} {'Œî':>6} {'Orig':>8} {'Cal':>8} {'Œî':>6} {'Orig':>8} {'Cal':>8} {'Œî':>6} {'N':>6}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for _, row in decision_analysis.iterrows():\n",
    "    thresh = row['Threshold']\n",
    "    sens_delta = row['Sensitivity_Cal'] - row['Sensitivity_Orig']\n",
    "    spec_delta = row['Specificity_Cal'] - row['Specificity_Orig']\n",
    "    ppv_delta = row['PPV_Cal'] - row['PPV_Orig']\n",
    "    \n",
    "    print(f\"{thresh:<10.1f} {row['Sensitivity_Orig']:>8.3f} {row['Sensitivity_Cal']:>8.3f} {sens_delta:>+6.3f} \"\n",
    "          f\"{row['Specificity_Orig']:>8.3f} {row['Specificity_Cal']:>8.3f} {spec_delta:>+6.3f} \"\n",
    "          f\"{row['PPV_Orig']:>8.3f} {row['PPV_Cal']:>8.3f} {ppv_delta:>+6.3f} {row['Decision_Changes']:>6.0f}\")\n",
    "\n",
    "print(f\"\\nTotal patients: {len(y_test)}\")\n",
    "print(f\"Actual malignant cases: {np.sum(y_test)}\")\n",
    "print(f\"Target prevalence: {target_prevalence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Clinical Implications\n",
    "\n",
    "This example demonstrates the value of rank-preserving calibration in medical applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLINICAL SUMMARY: Rank-Preserving Calibration for Medical Diagnosis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ SCENARIO:\")\n",
    "print(f\"   Model trained on population with {y_train.mean():.1%} disease prevalence\")\n",
    "print(f\"   Deployed in high-risk population with {target_prevalence:.1%} prevalence\")\n",
    "\n",
    "print(\"\\nüìä KEY RESULTS:\")\n",
    "print(f\"   ‚úì Maintained perfect patient ranking (AUC: {auc_original:.3f} ‚Üí {auc_calibrated:.3f})\")\n",
    "print(f\"   ‚úì Corrected prevalence estimate ({malignant_probs_original.mean():.3f} ‚Üí {malignant_probs_calibrated.mean():.3f})\")\n",
    "print(f\"   ‚úì Improved calibration (ECE: {ece_original:.4f} ‚Üí {ece_calibrated:.4f})\")\n",
    "print(f\"   ‚úì Better probability estimates for clinical decision making\")\n",
    "\n",
    "print(\"\\nüè• CLINICAL BENEFITS:\")\n",
    "print(\"   ‚Ä¢ Maintains relative risk ranking of patients\")\n",
    "print(\"   ‚Ä¢ Provides accurate absolute risk estimates\")\n",
    "print(\"   ‚Ä¢ Enables proper resource allocation in new populations\")\n",
    "print(\"   ‚Ä¢ Supports evidence-based clinical decision thresholds\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT CONSIDERATIONS:\")\n",
    "print(\"   ‚Ä¢ Requires reliable estimates of target population prevalence\")\n",
    "print(\"   ‚Ä¢ Should be validated on representative test data\")\n",
    "print(\"   ‚Ä¢ Consider confidence intervals for prevalence estimates\")\n",
    "print(\"   ‚Ä¢ Monitor performance in production deployment\")\n",
    "\n",
    "print(\"\\nüìà WHEN TO USE RANK-PRESERVING CALIBRATION:\")\n",
    "print(\"   ‚Ä¢ Deploying models across different populations\")\n",
    "print(\"   ‚Ä¢ When both ranking and absolute probabilities matter\")\n",
    "print(\"   ‚Ä¢ Resource allocation based on risk scores\")\n",
    "print(\"   ‚Ä¢ Clinical decision support systems\")\n",
    "\n",
    "# Show specific example of a clinical decision\n",
    "print(\"\\nüí° EXAMPLE CLINICAL DECISION (30% threshold):\")\n",
    "thresh_example = 0.3\n",
    "orig_flagged = np.sum(malignant_probs_original >= thresh_example)\n",
    "cal_flagged = np.sum(malignant_probs_calibrated >= thresh_example)\n",
    "changes = np.sum((malignant_probs_original >= thresh_example) != (malignant_probs_calibrated >= thresh_example))\n",
    "\n",
    "print(f\"   Original model: {orig_flagged} patients flagged for biopsy\")\n",
    "print(f\"   Calibrated model: {cal_flagged} patients flagged for biopsy\")\n",
    "print(f\"   Decision changes: {changes} patients ({100*changes/len(y_test):.1f}%)\")\n",
    "print(f\"   ‚Üí Better alignment with {target_prevalence:.0%} prevalence population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This example showed rank-preserving calibration for binary classification in a medical context. The same principles apply to:\n",
    "\n",
    "- **Multiclass medical diagnosis** (e.g., different types of skin lesions)\n",
    "- **Risk stratification** with multiple risk categories\n",
    "- **Treatment response prediction** across patient populations\n",
    "- **Biomarker discovery** with population-specific prevalences\n",
    "\n",
    "For more examples, see the other notebooks in this series:\n",
    "- Text classification with sentiment analysis\n",
    "- Image classification with vision models\n",
    "- Financial risk assessment\n",
    "- Survey reweighting applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}