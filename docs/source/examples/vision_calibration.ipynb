{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Calibrating Image Classification Models\n",
    "\n",
    "This example demonstrates rank-preserving calibration for computer vision applications using the handwritten digits dataset. We'll show how classifiers often suffer from overconfidence and how calibration can improve reliability while maintaining predictive performance.\n",
    "\n",
    "## Computer Vision Motivation\n",
    "\n",
    "Deep learning models for image classification face several calibration challenges:\n",
    "- **Overconfidence**: Neural networks often produce overly confident predictions\n",
    "- **Dataset shift**: Models trained on one dataset may be poorly calibrated on another\n",
    "- **Class imbalance**: Real-world deployments often have different class distributions than training data\n",
    "- **Safety-critical applications**: Medical imaging, autonomous vehicles require well-calibrated uncertainty\n",
    "\n",
    "Rank-preserving calibration maintains the model's ability to distinguish between images while providing more reliable probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.calibration import calibration_curve\nfrom scipy.stats import entropy\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import our calibration package - proper imports\nfrom rank_preserving_calibration import calibrate_dykstra\n\n# Set style for publication-quality plots\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (15, 10)\nplt.rcParams['font.size'] = 11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Business Context\n",
    "\n",
    "We'll use the handwritten digits dataset to simulate an optical character recognition (OCR) system deployed across different contexts:\n",
    "\n",
    "1. **Training environment**: Controlled lab conditions with balanced digit distribution\n",
    "2. **Production environment**: Real-world ZIP code processing with skewed digit frequencies\n",
    "\n",
    "The calibration challenge: ZIP codes contain certain digits more frequently (like 0, 1, 2) than others (like 8, 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the handwritten digits dataset\n",
    "print(\"üìä LOADING HANDWRITTEN DIGITS DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Feature dimensions: {X.shape[1]} (8x8 grayscale images)\")\n",
    "\n",
    "# Show class distribution in training data\n",
    "training_distribution = np.bincount(y) / len(y)\n",
    "print(f\"\\nTraining class distribution (balanced):\")\n",
    "for digit, freq in enumerate(training_distribution):\n",
    "    print(f\"  Digit {digit}: {freq:.3f} ({freq*100:.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Initial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "print(\"ü§ñ TRAINING COMPUTER VISION MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Help with any residual imbalance\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Baseline performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.3f}\")\n",
    "print(f\"Number of test samples: {len(y_test)}\")\n",
    "\n",
    "# Check prediction confidence\n",
    "max_probas = np.max(y_proba, axis=1)\n",
    "print(f\"\\nPrediction confidence statistics:\")\n",
    "print(f\"  Mean max probability: {np.mean(max_probas):.3f}\")\n",
    "print(f\"  Median max probability: {np.median(max_probas):.3f}\")\n",
    "print(f\"  Std max probability: {np.std(max_probas):.3f}\")\n",
    "print(f\"  Min max probability: {np.min(max_probas):.3f}\")\n",
    "print(f\"  Max max probability: {np.max(max_probas):.3f}\")\n",
    "\n",
    "# Show current marginals (class frequencies in predictions)\n",
    "current_marginals = np.mean(y_proba, axis=0)\n",
    "print(f\"\\nCurrent probability marginals:\")\n",
    "for digit, marginal in enumerate(current_marginals):\n",
    "    print(f\"  Digit {digit}: {marginal:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Deployment Scenario\n",
    "\n",
    "Now we simulate deploying this model to process ZIP codes, where digit frequencies follow real-world patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target distribution based on ZIP code digit frequencies\n",
    "# This simulates real-world deployment where certain digits are more common\n",
    "print(\"üåç PRODUCTION DEPLOYMENT SCENARIO: ZIP CODE PROCESSING\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Realistic ZIP code digit distribution (approximate US patterns)\n",
    "zip_code_distribution = np.array([\n",
    "    0.15,   # 0: Common in many ZIP codes\n",
    "    0.12,   # 1: Frequent \n",
    "    0.11,   # 2: Frequent\n",
    "    0.09,   # 3: Moderate\n",
    "    0.09,   # 4: Moderate  \n",
    "    0.08,   # 5: Moderate\n",
    "    0.10,   # 6: Moderate\n",
    "    0.08,   # 7: Less common\n",
    "    0.09,   # 8: Moderate\n",
    "    0.09    # 9: Moderate\n",
    "])\n",
    "\n",
    "print(\"Target distribution for ZIP code processing:\")\n",
    "for digit, freq in enumerate(zip_code_distribution):\n",
    "    print(f\"  Digit {digit}: {freq:.3f} ({freq*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribution shift from training:\")\n",
    "distribution_shift = zip_code_distribution - training_distribution\n",
    "for digit, shift in enumerate(distribution_shift):\n",
    "    direction = \"‚Üë\" if shift > 0 else \"‚Üì\" if shift < 0 else \"‚Üí\"\n",
    "    print(f\"  Digit {digit}: {shift:+.3f} {direction}\")\n",
    "\n",
    "# Business impact analysis\n",
    "print(f\"\\nüíº BUSINESS IMPACT ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Mail routing accuracy critical for delivery performance\")\n",
    "print(f\"   ‚Ä¢ Misclassified digits lead to delivery delays and customer complaints\")\n",
    "print(f\"   ‚Ä¢ Need probability estimates aligned with actual ZIP code patterns\")\n",
    "print(f\"   ‚Ä¢ Regulatory requirements for postal service reliability\")\n",
    "\n",
    "# Target marginals for calibration\n",
    "n_test_samples = len(y_test)\n",
    "target_marginals = zip_code_distribution * n_test_samples\n",
    "\n",
    "print(f\"\\nCalibration targets:\")\n",
    "print(f\"  Total samples: {n_test_samples}\")\n",
    "print(f\"  Target marginals: {target_marginals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank-Preserving Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rank-preserving calibration\n",
    "print(\"üîß APPLYING RANK-PRESERVING CALIBRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calibrate probabilities\n",
    "result = calibrate_dykstra(\n",
    "    P=y_proba, \n",
    "    M=target_marginals,\n",
    "    max_iters=2000,\n",
    "    tol=1e-7,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "y_proba_calibrated = result.Q\n",
    "print(f\"\\nCalibration completed successfully!\")\n",
    "print(f\"  Converged: {result.converged}\")\n",
    "print(f\"  Iterations: {result.iterations}\")\n",
    "print(f\"  Final objective: {result.objective:.2e}\")\n",
    "\n",
    "# Verify calibration worked\n",
    "calibrated_marginals = np.sum(y_proba_calibrated, axis=0)\n",
    "print(f\"\\n‚úÖ CALIBRATION VERIFICATION:\")\n",
    "print(f\"Target vs Achieved marginals:\")\n",
    "for digit in range(10):\n",
    "    target = target_marginals[digit]\n",
    "    achieved = calibrated_marginals[digit]\n",
    "    error = abs(achieved - target)\n",
    "    print(f\"  Digit {digit}: {target:.1f} ‚Üí {achieved:.1f} (error: {error:.2e})\")\n",
    "\n",
    "max_marginal_error = np.max(np.abs(calibrated_marginals - target_marginals))\n",
    "print(f\"\\nMaximum marginal constraint violation: {max_marginal_error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of calibration impact\n",
    "print(\"üìà CALIBRATION IMPACT ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Ranking preservation\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Check if rankings are preserved for each sample\n",
    "spearman_correlations = []\n",
    "for i in range(len(y_test)):\n",
    "    corr, _ = spearmanr(y_proba[i], y_proba_calibrated[i])\n",
    "    spearman_correlations.append(corr)\n",
    "\n",
    "spearman_correlations = np.array(spearman_correlations)\n",
    "perfect_rank_preservation = np.sum(np.isclose(spearman_correlations, 1.0, atol=1e-10))\n",
    "\n",
    "print(f\"RANK PRESERVATION ANALYSIS:\")\n",
    "print(f\"  Perfect rank preservation: {perfect_rank_preservation}/{len(y_test)} samples\")\n",
    "print(f\"  Mean Spearman correlation: {np.mean(spearman_correlations):.6f}\")\n",
    "print(f\"  Min Spearman correlation: {np.min(spearman_correlations):.6f}\")\n",
    "print(f\"  Samples with correlation < 0.999: {np.sum(spearman_correlations < 0.999)}\")\n",
    "\n",
    "# 2. Prediction changes\n",
    "original_predictions = np.argmax(y_proba, axis=1)\n",
    "calibrated_predictions = np.argmax(y_proba_calibrated, axis=1)\n",
    "prediction_changes = np.sum(original_predictions != calibrated_predictions)\n",
    "\n",
    "print(f\"\\nPREDICTION IMPACT:\")\n",
    "print(f\"  Total prediction changes: {prediction_changes}/{len(y_test)}\")\n",
    "print(f\"  Prediction stability: {(1 - prediction_changes/len(y_test))*100:.1f}%\")\n",
    "\n",
    "if prediction_changes > 0:\n",
    "    changed_indices = np.where(original_predictions != calibrated_predictions)[0]\n",
    "    print(f\"  Changed predictions involve digits:\")\n",
    "    for idx in changed_indices[:5]:  # Show first 5 changes\n",
    "        orig = original_predictions[idx]\n",
    "        calib = calibrated_predictions[idx]\n",
    "        true_label = y_test[idx]\n",
    "        print(f\"    Sample {idx}: {orig} ‚Üí {calib} (true: {true_label})\")\n",
    "\n",
    "# 3. Accuracy comparison\n",
    "original_accuracy = accuracy_score(y_test, original_predictions)\n",
    "calibrated_accuracy = accuracy_score(y_test, calibrated_predictions)\n",
    "\n",
    "print(f\"\\nACCURACY COMPARISON:\")\n",
    "print(f\"  Original accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"  Calibrated accuracy: {calibrated_accuracy:.4f}\")\n",
    "print(f\"  Accuracy change: {calibrated_accuracy - original_accuracy:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# 1. Marginal comparison\n",
    "x_pos = np.arange(10)\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x_pos - width, training_distribution, width, \n",
    "               label='Training', alpha=0.8, color='skyblue')\n",
    "axes[0, 0].bar(x_pos, current_marginals, width, \n",
    "               label='Original Model', alpha=0.8, color='orange')\n",
    "axes[0, 0].bar(x_pos + width, zip_code_distribution, width, \n",
    "               label='Target (ZIP codes)', alpha=0.8, color='green')\n",
    "axes[0, 0].set_xlabel('Digit')\n",
    "axes[0, 0].set_ylabel('Probability Mass')\n",
    "axes[0, 0].set_title('Distribution Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "\n",
    "# 2. Calibration accuracy per digit\n",
    "achieved_distribution = calibrated_marginals / n_test_samples\n",
    "calibration_errors = np.abs(achieved_distribution - zip_code_distribution)\n",
    "\n",
    "bars = axes[0, 1].bar(x_pos, calibration_errors, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Digit')\n",
    "axes[0, 1].set_ylabel('Absolute Error')\n",
    "axes[0, 1].set_title('Calibration Accuracy by Digit')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# 3. Probability change distribution\n",
    "prob_changes = y_proba_calibrated - y_proba\n",
    "axes[0, 2].hist(prob_changes.flatten(), bins=50, alpha=0.7, \n",
    "                density=True, color='purple')\n",
    "axes[0, 2].axvline(0, color='black', linestyle='--')\n",
    "axes[0, 2].set_xlabel('Probability Change')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Distribution of Probability Changes')\n",
    "\n",
    "# 4. Reliability diagram (calibration curve)\n",
    "def plot_reliability_diagram(y_true, y_proba, ax, title):\n",
    "    n_classes = y_proba.shape[1]\n",
    "    for digit in range(n_classes):\n",
    "        y_binary = (y_true == digit).astype(int)\n",
    "        if np.sum(y_binary) > 0:  # Only plot if class exists\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_binary, y_proba[:, digit], n_bins=10\n",
    "            )\n",
    "            ax.plot(mean_predicted_value, fraction_of_positives, \n",
    "                   's-', label=f'Digit {digit}', color=colors[digit], alpha=0.7)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k:', label='Perfect calibration')\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plot_reliability_diagram(y_test, y_proba, axes[1, 0], 'Original Model Calibration')\n",
    "plot_reliability_diagram(y_test, y_proba_calibrated, axes[1, 1], 'Calibrated Model')\n",
    "\n",
    "# 5. Per-class probability changes\n",
    "prob_changes = y_proba_calibrated - y_proba\n",
    "for digit in range(10):\n",
    "    axes[1, 2].hist(prob_changes[:, digit], bins=20, alpha=0.7, \n",
    "                   label=f'Digit {digit}', color=colors[digit], density=True)\n",
    "\n",
    "axes[1, 2].axvline(0, color='black', linestyle='--')\n",
    "axes[1, 2].set_xlabel('Probability Change')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "axes[1, 2].set_title('Distribution of Changes by Digit')\n",
    "axes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 6. Confusion matrix comparison\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm_original = confusion_matrix(y_test, original_predictions)\n",
    "cm_calibrated = confusion_matrix(y_test, calibrated_predictions)\n",
    "\n",
    "# Normalize for better comparison\n",
    "cm_original_norm = cm_original.astype('float') / cm_original.sum(axis=1)[:, np.newaxis]\n",
    "cm_calibrated_norm = cm_calibrated.astype('float') / cm_calibrated.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "im1 = axes[2, 0].imshow(cm_original_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[2, 0].set_title('Original Predictions')\n",
    "axes[2, 0].set_xlabel('Predicted Digit')\n",
    "axes[2, 0].set_ylabel('True Digit')\n",
    "axes[2, 0].set_xticks(range(10))\n",
    "axes[2, 0].set_yticks(range(10))\n",
    "\n",
    "im2 = axes[2, 1].imshow(cm_calibrated_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[2, 1].set_title('Calibrated Predictions')\n",
    "axes[2, 1].set_xlabel('Predicted Digit')\n",
    "axes[2, 1].set_ylabel('True Digit')\n",
    "axes[2, 1].set_xticks(range(10))\n",
    "axes[2, 1].set_yticks(range(10))\n",
    "\n",
    "# 7. Difference in confusion matrices\n",
    "cm_diff = cm_calibrated_norm - cm_original_norm\n",
    "im3 = axes[2, 2].imshow(cm_diff, interpolation='nearest', cmap=plt.cm.RdBu, \n",
    "                        vmin=-np.max(np.abs(cm_diff)), vmax=np.max(np.abs(cm_diff)))\n",
    "axes[2, 2].set_title('Difference (Calibrated - Original)')\n",
    "axes[2, 2].set_xlabel('Predicted Digit')\n",
    "axes[2, 2].set_ylabel('True Digit')\n",
    "axes[2, 2].set_xticks(range(10))\n",
    "axes[2, 2].set_yticks(range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Impact Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact analysis for computer vision deployment\n",
    "print(\"üí∞ BUSINESS IMPACT ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simulate business metrics\n",
    "n_daily_images = 50000  # Images processed per day\n",
    "n_annual_images = n_daily_images * 365\n",
    "\n",
    "# Cost parameters\n",
    "cost_per_misclassification = 2.50  # Cost of routing error\n",
    "cost_per_manual_review = 0.15     # Human verification cost\n",
    "revenue_per_correct_classification = 0.05  # Processing fee\n",
    "\n",
    "# Calculate error rates and associated costs\n",
    "original_error_rate = 1 - original_accuracy\n",
    "calibrated_error_rate = 1 - calibrated_accuracy\n",
    "\n",
    "print(f\"üìä OPERATIONAL METRICS:\")\n",
    "print(f\"   ‚Ä¢ Daily image volume: {n_daily_images:,}\")\n",
    "print(f\"   ‚Ä¢ Annual image volume: {n_annual_images:,}\")\n",
    "print(f\"   ‚Ä¢ Original error rate: {original_error_rate:.4f} ({original_error_rate*100:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ Calibrated error rate: {calibrated_error_rate:.4f} ({calibrated_error_rate*100:.2f}%)\")\n",
    "\n",
    "# Annual cost comparison\n",
    "original_annual_errors = n_annual_images * original_error_rate\n",
    "calibrated_annual_errors = n_annual_images * calibrated_error_rate\n",
    "error_reduction = original_annual_errors - calibrated_annual_errors\n",
    "\n",
    "original_error_cost = original_annual_errors * cost_per_misclassification\n",
    "calibrated_error_cost = calibrated_annual_errors * cost_per_misclassification\n",
    "annual_cost_savings = original_error_cost - calibrated_error_cost\n",
    "\n",
    "print(f\"\\nüíµ ANNUAL FINANCIAL IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Original annual errors: {original_annual_errors:,.0f}\")\n",
    "print(f\"   ‚Ä¢ Calibrated annual errors: {calibrated_annual_errors:,.0f}\")\n",
    "print(f\"   ‚Ä¢ Error reduction: {error_reduction:,.0f} ({(error_reduction/original_annual_errors)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Annual cost savings: ${annual_cost_savings:,.2f}\")\n",
    "\n",
    "# Confidence-based routing analysis\n",
    "# Use calibrated probabilities to determine which images need manual review\n",
    "confidence_threshold = 0.95\n",
    "max_calibrated_probs = np.max(y_proba_calibrated, axis=1)\n",
    "max_original_probs = np.max(y_proba, axis=1)\n",
    "\n",
    "high_conf_original = np.sum(max_original_probs >= confidence_threshold)\n",
    "high_conf_calibrated = np.sum(max_calibrated_probs >= confidence_threshold)\n",
    "\n",
    "manual_review_original = len(y_test) - high_conf_original\n",
    "manual_review_calibrated = len(y_test) - high_conf_calibrated\n",
    "\n",
    "print(f\"\\nüîç CONFIDENCE-BASED ROUTING (threshold = {confidence_threshold}):\")\n",
    "print(f\"   ‚Ä¢ Original: {high_conf_original}/{len(y_test)} auto-processed\")\n",
    "print(f\"   ‚Ä¢ Calibrated: {high_conf_calibrated}/{len(y_test)} auto-processed\")\n",
    "print(f\"   ‚Ä¢ Manual review reduction: {manual_review_original - manual_review_calibrated} samples\")\n",
    "\n",
    "# Scale to annual volume\n",
    "annual_manual_original = (manual_review_original / len(y_test)) * n_annual_images\n",
    "annual_manual_calibrated = (manual_review_calibrated / len(y_test)) * n_annual_images\n",
    "annual_manual_savings = annual_manual_original - annual_manual_calibrated\n",
    "\n",
    "manual_cost_savings = annual_manual_savings * cost_per_manual_review\n",
    "\n",
    "print(f\"   ‚Ä¢ Annual manual review cost savings: ${manual_cost_savings:,.2f}\")\n",
    "\n",
    "# Total business impact\n",
    "total_annual_savings = annual_cost_savings + manual_cost_savings\n",
    "roi_percentage = (total_annual_savings / (n_annual_images * revenue_per_correct_classification)) * 100\n",
    "\n",
    "print(f\"\\nüéØ TOTAL BUSINESS IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Total annual savings: ${total_annual_savings:,.2f}\")\n",
    "print(f\"   ‚Ä¢ ROI on processing volume: {roi_percentage:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Payback period: Immediate (operational efficiency)\")\n",
    "\n",
    "# Deployment recommendations\n",
    "print(f\"\\nüöÄ DEPLOYMENT RECOMMENDATIONS:\")\n",
    "deployment_recommendations = [\n",
    "    \"Implement calibrated model in production OCR pipeline\",\n",
    "    \"Use confidence thresholds for automated vs manual routing\",\n",
    "    \"Monitor real-world digit distribution shifts over time\",\n",
    "    \"Establish periodic recalibration based on seasonal patterns\",\n",
    "    \"Extend calibration to other postal code formats (international)\",\n",
    "    \"Consider ensemble methods for further accuracy improvements\",\n",
    "    \"Implement A/B testing framework for continuous optimization\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(deployment_recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# Risk assessment\n",
    "print(f\"\\n‚ö†Ô∏è  IMPLEMENTATION CONSIDERATIONS:\")\n",
    "considerations = [\n",
    "    \"Validate calibration on recent ZIP code data before deployment\",\n",
    "    \"Monitor for concept drift in digit distribution patterns\",\n",
    "    \"Ensure compliance with postal service accuracy requirements\",\n",
    "    \"Plan for emergency fallback to original model if needed\",\n",
    "    \"Train operations team on new confidence-based routing logic\"\n",
    "]\n",
    "\n",
    "for consideration in considerations:\n",
    "    print(f\"   ‚Ä¢ {consideration}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE GUARANTEES:\")\n",
    "print(f\"   ‚Ä¢ Rank preservation: Perfect ordering maintained (correlation = {np.mean(spearman_correlations):.6f})\")\n",
    "print(f\"   ‚Ä¢ Constraint satisfaction: Maximum marginal error < {np.max(np.abs(calibrated_marginals - target_marginals)):.2e}\")\n",
    "print(f\"   ‚Ä¢ Convergence: Algorithm converged in {result.iterations} iterations\")\n",
    "print(f\"   ‚Ä¢ Stability: {(1 - prediction_changes/n_test_samples)*100:.1f}% of predictions unchanged\")\n",
    "\n",
    "# Calibration quality check\n",
    "final_row_errors = np.abs(y_proba_calibrated.sum(axis=1) - 1.0)\n",
    "final_col_errors = np.abs(calibrated_marginals - target_marginals)\n",
    "\n",
    "print(f\"\\n‚úÖ QUALITY ASSURANCE:\")\n",
    "print(f\"   ‚Ä¢ Maximum row constraint violation: {np.max(final_row_errors):.2e}\")\n",
    "print(f\"   ‚Ä¢ Maximum column constraint violation: {np.max(final_col_errors):.2e}\")\n",
    "print(f\"   ‚Ä¢ Calibration converged: {'Yes' if result.converged else 'No'}\")\n",
    "print(f\"   ‚Ä¢ All mathematical constraints satisfied within numerical precision\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Deploy calibrated model in staging environment\")\n",
    "print(f\"   2. Monitor real-world performance against baseline\")\n",
    "print(f\"   3. Collect feedback on decision quality improvements\")\n",
    "print(f\"   4. Establish periodic recalibration protocols\")\n",
    "print(f\"   5. Scale to additional computer vision applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This example demonstrated rank-preserving calibration for computer vision applications. The same principles apply broadly to:\n",
    "\n",
    "- **Medical imaging** with population-specific disease prevalence\n",
    "- **Autonomous systems** requiring calibrated uncertainty for safety\n",
    "- **Industrial automation** with domain-specific defect rates\n",
    "- **Content moderation** across platforms with different content distributions\n",
    "- **Retail applications** with store or region-specific product mix\n",
    "\n",
    "The key insight is that rank-preserving calibration maintains the model's core discriminative ability while adapting the probability estimates to match deployment conditions.\n",
    "\n",
    "For more examples in different domains, see the other notebooks:\n",
    "- Medical diagnosis with clinical population shifts\n",
    "- Text classification with domain adaptation\n",
    "- Financial risk assessment with portfolio-specific distributions\n",
    "- Survey reweighting for demographic correction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}