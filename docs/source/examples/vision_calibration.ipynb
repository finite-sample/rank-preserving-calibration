{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computer Vision: Calibrating Image Classification Models\n",
        "\n",
        "This example demonstrates rank-preserving calibration for computer vision applications using the handwritten digits dataset. We'll show how classifiers often suffer from overconfidence and how calibration can improve reliability while maintaining predictive performance.\n",
        "\n",
        "## Computer Vision Motivation\n",
        "\n",
        "Deep learning models for image classification face several calibration challenges:\n",
        "- **Overconfidence**: Neural networks often produce overly confident predictions\n",
        "- **Dataset shift**: Models trained on one dataset may be poorly calibrated on another\n",
        "- **Class imbalance**: Real-world deployments often have different class distributions than training data\n",
        "- **Safety-critical applications**: Medical imaging, autonomous vehicles require well-calibrated uncertainty\n",
        "\n",
        "Rank-preserving calibration maintains the model's ability to distinguish between images while providing more reliable probability estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our calibration package - proper imports\n",
        "from rank_preserving_calibration import calibrate_dykstra\n",
        "\n",
        "# Set style for publication-quality plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Business Context\n",
        "\n",
        "We'll use the handwritten digits dataset to simulate an optical character recognition (OCR) system deployed across different contexts:\n",
        "\n",
        "1. **Training environment**: Controlled lab conditions with balanced digit distribution\n",
        "2. **Production environment**: Real-world ZIP code processing with skewed digit frequencies\n",
        "\n",
        "The calibration challenge: ZIP codes contain certain digits more frequently (like 0, 1, 2) than others (like 8, 9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the handwritten digits dataset\n",
        "print(\"\ud83d\udcca LOADING HANDWRITTEN DIGITS DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Feature dimensions: {X.shape[1]} (8x8 grayscale images)\")\n",
        "\n",
        "# Show class distribution in training data\n",
        "training_distribution = np.bincount(y) / len(y)\n",
        "print(\"\\nTraining class distribution (balanced):\")\n",
        "for digit, freq in enumerate(training_distribution):\n",
        "    print(f\"  Digit {digit}: {freq:.3f} ({freq*100:.1f}%)\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Initial Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a Random Forest classifier\n",
        "print(\"\ud83e\udd16 TRAINING COMPUTER VISION MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'  # Help with any residual imbalance\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predictions and probabilities\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Baseline performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.3f}\")\n",
        "print(f\"Number of test samples: {len(y_test)}\")\n",
        "\n",
        "# Check prediction confidence\n",
        "max_probas = np.max(y_proba, axis=1)\n",
        "print(\"\\nPrediction confidence statistics:\")\n",
        "print(f\"  Mean max probability: {np.mean(max_probas):.3f}\")\n",
        "print(f\"  Median max probability: {np.median(max_probas):.3f}\")\n",
        "print(f\"  Std max probability: {np.std(max_probas):.3f}\")\n",
        "print(f\"  Min max probability: {np.min(max_probas):.3f}\")\n",
        "print(f\"  Max max probability: {np.max(max_probas):.3f}\")\n",
        "\n",
        "# Show current marginals (class frequencies in predictions)\n",
        "current_marginals = np.mean(y_proba, axis=0)\n",
        "print(\"\\nCurrent probability marginals:\")\n",
        "for digit, marginal in enumerate(current_marginals):\n",
        "    print(f\"  Digit {digit}: {marginal:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Deployment Scenario\n",
        "\n",
        "Now we simulate deploying this model to process ZIP codes, where digit frequencies follow real-world patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define target distribution based on ZIP code digit frequencies\n",
        "# This simulates real-world deployment where certain digits are more common\n",
        "print(\"\ud83c\udf0d PRODUCTION DEPLOYMENT SCENARIO: ZIP CODE PROCESSING\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Realistic ZIP code digit distribution (approximate US patterns)\n",
        "zip_code_distribution = np.array([\n",
        "    0.15,   # 0: Common in many ZIP codes\n",
        "    0.12,   # 1: Frequent\n",
        "    0.11,   # 2: Frequent\n",
        "    0.09,   # 3: Moderate\n",
        "    0.09,   # 4: Moderate\n",
        "    0.08,   # 5: Moderate\n",
        "    0.10,   # 6: Moderate\n",
        "    0.08,   # 7: Less common\n",
        "    0.09,   # 8: Moderate\n",
        "    0.09    # 9: Moderate\n",
        "])\n",
        "\n",
        "print(\"Target distribution for ZIP code processing:\")\n",
        "for digit, freq in enumerate(zip_code_distribution):\n",
        "    print(f\"  Digit {digit}: {freq:.3f} ({freq*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDistribution shift from training:\")\n",
        "distribution_shift = zip_code_distribution - training_distribution\n",
        "for digit, shift in enumerate(distribution_shift):\n",
        "    direction = \"\u2191\" if shift > 0 else \"\u2193\" if shift < 0 else \"\u2192\"\n",
        "    print(f\"  Digit {digit}: {shift:+.3f} {direction}\")\n",
        "\n",
        "# Business impact analysis\n",
        "print(\"\\n\ud83d\udcbc BUSINESS IMPACT ANALYSIS:\")\n",
        "print(\"   \u2022 Mail routing accuracy critical for delivery performance\")\n",
        "print(\"   \u2022 Misclassified digits lead to delivery delays and customer complaints\")\n",
        "print(\"   \u2022 Need probability estimates aligned with actual ZIP code patterns\")\n",
        "print(\"   \u2022 Regulatory requirements for postal service reliability\")\n",
        "\n",
        "# Target marginals for calibration\n",
        "n_test_samples = len(y_test)\n",
        "target_marginals = zip_code_distribution * n_test_samples\n",
        "\n",
        "print(\"\\nCalibration targets:\")\n",
        "print(f\"  Total samples: {n_test_samples}\")\n",
        "print(f\"  Target marginals: {target_marginals}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rank-Preserving Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Apply rank-preserving calibration\\nprint(\\\"\ud83d\udd27 APPLYING RANK-PRESERVING CALIBRATION\\\")\\nprint(\\\"=\\\"*50)\\n\\n# Calibrate probabilities\\nresult = calibrate_dykstra(\\n    P=y_proba,\\n    M=target_marginals,\\n    max_iters=2000,\\n    tol=1e-7,\\n    verbose=True\\n)\\n\\ny_proba_calibrated = result.Q\\n\\n# Check if algorithm produced valid probabilities\\nhas_negative = np.any(y_proba_calibrated < 0)\\nhas_over_one = np.any(y_proba_calibrated > 1)\\nrow_sums = np.sum(y_proba_calibrated, axis=1)\\nrow_sums_ok = np.allclose(row_sums, 1.0, atol=1e-10)\\n\\n# Only apply minimal fixes if absolutely necessary\\nif has_negative or has_over_one or not row_sums_ok:\\n    print(f\\\"\\\\n\u26a0\ufe0f  WARNING: Algorithm produced invalid probabilities!\\\")\\n    if has_negative:\\n        print(f\\\"   \u2022 Min probability: {np.min(y_proba_calibrated):.6f}\\\")\\n    if has_over_one:\\n        print(f\\\"   \u2022 Max probability: {np.max(y_proba_calibrated):.6f}\\\")\\n    if not row_sums_ok:\\n        print(f\\\"   \u2022 Row sum range: [{np.min(row_sums):.6f}, {np.max(row_sums):.6f}]\\\")\\n    \\n    y_proba_calibrated_original = y_proba_calibrated.copy()\\n    y_proba_calibrated = np.clip(y_proba_calibrated, 1e-12, 1.0)\\n    y_proba_calibrated = y_proba_calibrated / np.sum(y_proba_calibrated, axis=1, keepdims=True)\\n    print(\\\"   \u2022 Applied clipping and renormalization fix\\\")\\n\\nprint(\\\"\\\\n\ud83d\udcca CALIBRATION ALGORITHM STATUS:\\\")\\nprint(f\\\"  Converged: {result.converged}\\\")\\nprint(f\\\"  Iterations: {result.iterations}\\\")\\nprint(f\\\"  Final change: {result.final_change:.2e}\\\")\\nif not result.converged:\\n    print(\\\"  \u26a0\ufe0f  Algorithm did not converge - results may be suboptimal\\\")\\n\\n# Verify calibration worked\\ncalibrated_marginals = np.sum(y_proba_calibrated, axis=0)\\nprint(\\\"\\\\n\u2705 CALIBRATION VERIFICATION:\\\")\\nprint(\\\"Target vs Achieved marginals:\\\")\\nfor digit in range(10):\\n    target = target_marginals[digit]\\n    achieved = calibrated_marginals[digit]\\n    error = abs(achieved - target)\\n    print(f\\\"  Digit {digit}: {target:.1f} \u2192 {achieved:.1f} (error: {error:.2e})\\\")\\n\\nmax_marginal_error = np.max(np.abs(calibrated_marginals - target_marginals))\\nprint(f\\\"\\\\nMaximum marginal constraint violation: {max_marginal_error:.2e}\\\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Impact Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Comprehensive analysis of calibration impact\\nprint(\\\"\ud83d\udcc8 CALIBRATION IMPACT ANALYSIS\\\")\\nprint(\\\"=\\\"*40)\\n\\n# 1. Ranking preservation\\nfrom scipy.stats import spearmanr\\n\\n# Check if rankings are preserved for each sample\\nspearman_correlations = []\\nfor i in range(len(y_test)):\\n    corr, _ = spearmanr(y_proba[i], y_proba_calibrated[i])\\n    spearman_correlations.append(corr)\\n\\nspearman_correlations = np.array(spearman_correlations)\\nperfect_rank_preservation = np.sum(np.isclose(spearman_correlations, 1.0, atol=1e-10))\\n\\nprint(\\\"RANK PRESERVATION ANALYSIS:\\\")\\nprint(f\\\"  Perfect rank preservation: {perfect_rank_preservation}/{len(y_test)} samples\\\")\\nprint(f\\\"  Mean Spearman correlation: {np.mean(spearman_correlations):.6f}\\\")\\nprint(f\\\"  Min Spearman correlation: {np.min(spearman_correlations):.6f}\\\")\\nprint(f\\\"  Samples with correlation < 0.999: {np.sum(spearman_correlations < 0.999)}\\\")\\n\\n# 2. Prediction changes\\noriginal_predictions = np.argmax(y_proba, axis=1)\\ncalibrated_predictions = np.argmax(y_proba_calibrated, axis=1)\\nprediction_changes = np.sum(original_predictions != calibrated_predictions)\\n\\nprint(\\\"\\\\nPREDICTION IMPACT:\\\")\\nprint(f\\\"  Total prediction changes: {prediction_changes}/{len(y_test)}\\\")\\nprint(f\\\"  Prediction stability: {(1 - prediction_changes/len(y_test))*100:.1f}%\\\")\\n\\nif prediction_changes > 0:\\n    changed_indices = np.where(original_predictions != calibrated_predictions)[0]\\n    print(\\\"  Changed predictions involve digits:\\\")\\n    for idx in changed_indices[:5]:  # Show first 5 changes\\n        orig = original_predictions[idx]\\n        calib = calibrated_predictions[idx]\\n        true_label = y_test[idx]\\n        print(f\\\"    Sample {idx}: {orig} \u2192 {calib} (true: {true_label})\\\")\\n\\n# 3. Accuracy comparison\\noriginal_accuracy = accuracy_score(y_test, original_predictions)\\ncalibrated_accuracy = accuracy_score(y_test, calibrated_predictions)\\n\\nprint(\\\"\\\\nACCURACY COMPARISON:\\\")\\nprint(f\\\"  Original accuracy: {original_accuracy:.4f}\\\")\\nprint(f\\\"  Calibrated accuracy: {calibrated_accuracy:.4f}\\\")\\nprint(f\\\"  Accuracy change: {calibrated_accuracy - original_accuracy:+.4f}\\\")\\n\\n# 4. Probability quality assessment\\ntry:\\n    from sklearn.metrics import log_loss\\n    original_logloss = log_loss(y_test, y_proba)\\n    calibrated_logloss = log_loss(y_test, y_proba_calibrated)\\n    print(\\\"\\\\nPROBABILITY QUALITY:\\\")\\n    print(f\\\"  Original log loss: {original_logloss:.4f}\\\")\\n    print(f\\\"  Calibrated log loss: {calibrated_logloss:.4f}\\\")\\n    print(f\\\"  Log loss change: {calibrated_logloss - original_logloss:+.4f}\\\")\\nexcept ValueError as e:\\n    print(f\\\"\\\\n\u26a0\ufe0f  Log loss calculation failed: {e}\\\")\\n\\n# 5. Overall assessment\\nprint(\\\"\\\\n\ud83c\udfaf CALIBRATION PERFORMANCE SUMMARY:\\\")\\nif np.mean(spearman_correlations) < 0.95:\\n    print(f\\\"   \u274c Poor rank preservation (correlation = {np.mean(spearman_correlations):.3f})\\\")\\nelif np.mean(spearman_correlations) < 0.99:\\n    print(f\\\"   \u26a0\ufe0f  Moderate rank preservation (correlation = {np.mean(spearman_correlations):.3f})\\\")\\nelse:\\n    print(f\\\"   \u2705 Excellent rank preservation (correlation = {np.mean(spearman_correlations):.3f})\\\")\\n\\nif calibrated_accuracy < original_accuracy - 0.02:\\n    print(f\\\"   \u274c Significant accuracy degradation ({calibrated_accuracy - original_accuracy:+.3f})\\\")\\nelif calibrated_accuracy < original_accuracy - 0.005:\\n    print(f\\\"   \u26a0\ufe0f  Minor accuracy decrease ({calibrated_accuracy - original_accuracy:+.3f})\\\")\\nelse:\\n    print(f\\\"   \u2705 Accuracy maintained ({calibrated_accuracy - original_accuracy:+.3f})\\\")\\n\\nif not result.converged:\\n    print(f\\\"   \u274c Algorithm failed to converge after {result.iterations} iterations\\\")\\nelse:\\n    print(f\\\"   \u2705 Algorithm converged in {result.iterations} iterations\\\")\\n\\nif has_negative or has_over_one or not row_sums_ok:\\n    print(f\\\"   \u26a0\ufe0f  Required probability corrections due to algorithm issues\\\")\\nelse:\\n    print(f\\\"   \u2705 Algorithm produced valid probabilities\\\")\""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "\n",
        "# 1. Marginal comparison\n",
        "x_pos = np.arange(10)\n",
        "width = 0.25\n",
        "\n",
        "axes[0, 0].bar(x_pos - width, training_distribution, width,\n",
        "               label='Training', alpha=0.8, color='skyblue')\n",
        "axes[0, 0].bar(x_pos, current_marginals, width,\n",
        "               label='Original Model', alpha=0.8, color='orange')\n",
        "axes[0, 0].bar(x_pos + width, zip_code_distribution, width,\n",
        "               label='Target (ZIP codes)', alpha=0.8, color='green')\n",
        "axes[0, 0].set_xlabel('Digit')\n",
        "axes[0, 0].set_ylabel('Probability Mass')\n",
        "axes[0, 0].set_title('Distribution Comparison')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "\n",
        "# 2. Calibration accuracy per digit\n",
        "achieved_distribution = calibrated_marginals / n_test_samples\n",
        "calibration_errors = np.abs(achieved_distribution - zip_code_distribution)\n",
        "\n",
        "bars = axes[0, 1].bar(x_pos, calibration_errors, color=colors, alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Digit')\n",
        "axes[0, 1].set_ylabel('Absolute Error')\n",
        "axes[0, 1].set_title('Calibration Accuracy by Digit')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_yscale('log')\n",
        "\n",
        "# 3. Probability change distribution\n",
        "prob_changes = y_proba_calibrated - y_proba\n",
        "axes[0, 2].hist(prob_changes.flatten(), bins=50, alpha=0.7,\n",
        "                density=True, color='purple')\n",
        "axes[0, 2].axvline(0, color='black', linestyle='--')\n",
        "axes[0, 2].set_xlabel('Probability Change')\n",
        "axes[0, 2].set_ylabel('Density')\n",
        "axes[0, 2].set_title('Distribution of Probability Changes')\n",
        "\n",
        "# 4. Reliability diagram (calibration curve)\n",
        "def plot_reliability_diagram(y_true, y_proba, ax, title):\n",
        "    n_classes = y_proba.shape[1]\n",
        "    for digit in range(n_classes):\n",
        "        y_binary = (y_true == digit).astype(int)\n",
        "        if np.sum(y_binary) > 0:  # Only plot if class exists\n",
        "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "                y_binary, y_proba[:, digit], n_bins=10\n",
        "            )\n",
        "            ax.plot(mean_predicted_value, fraction_of_positives,\n",
        "                   's-', label=f'Digit {digit}', color=colors[digit], alpha=0.7)\n",
        "\n",
        "    ax.plot([0, 1], [0, 1], 'k:', label='Perfect calibration')\n",
        "    ax.set_xlabel('Mean Predicted Probability')\n",
        "    ax.set_ylabel('Fraction of Positives')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plot_reliability_diagram(y_test, y_proba, axes[1, 0], 'Original Model Calibration')\n",
        "plot_reliability_diagram(y_test, y_proba_calibrated, axes[1, 1], 'Calibrated Model')\n",
        "\n",
        "# 5. Per-class probability changes\n",
        "prob_changes = y_proba_calibrated - y_proba\n",
        "for digit in range(10):\n",
        "    axes[1, 2].hist(prob_changes[:, digit], bins=20, alpha=0.7,\n",
        "                   label=f'Digit {digit}', color=colors[digit], density=True)\n",
        "\n",
        "axes[1, 2].axvline(0, color='black', linestyle='--')\n",
        "axes[1, 2].set_xlabel('Probability Change')\n",
        "axes[1, 2].set_ylabel('Density')\n",
        "axes[1, 2].set_title('Distribution of Changes by Digit')\n",
        "axes[1, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# 6. Confusion matrix comparison\n",
        "import seaborn as sns\n",
        "\n",
        "cm_original = confusion_matrix(y_test, original_predictions)\n",
        "cm_calibrated = confusion_matrix(y_test, calibrated_predictions)\n",
        "\n",
        "# Normalize for better comparison\n",
        "cm_original_norm = cm_original.astype('float') / cm_original.sum(axis=1)[:, np.newaxis]\n",
        "cm_calibrated_norm = cm_calibrated.astype('float') / cm_calibrated.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "im1 = axes[2, 0].imshow(cm_original_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "axes[2, 0].set_title('Original Predictions')\n",
        "axes[2, 0].set_xlabel('Predicted Digit')\n",
        "axes[2, 0].set_ylabel('True Digit')\n",
        "axes[2, 0].set_xticks(range(10))\n",
        "axes[2, 0].set_yticks(range(10))\n",
        "\n",
        "im2 = axes[2, 1].imshow(cm_calibrated_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "axes[2, 1].set_title('Calibrated Predictions')\n",
        "axes[2, 1].set_xlabel('Predicted Digit')\n",
        "axes[2, 1].set_ylabel('True Digit')\n",
        "axes[2, 1].set_xticks(range(10))\n",
        "axes[2, 1].set_yticks(range(10))\n",
        "\n",
        "# 7. Difference in confusion matrices\n",
        "cm_diff = cm_calibrated_norm - cm_original_norm\n",
        "im3 = axes[2, 2].imshow(cm_diff, interpolation='nearest', cmap=plt.cm.RdBu,\n",
        "                        vmin=-np.max(np.abs(cm_diff)), vmax=np.max(np.abs(cm_diff)))\n",
        "axes[2, 2].set_title('Difference (Calibrated - Original)')\n",
        "axes[2, 2].set_xlabel('Predicted Digit')\n",
        "axes[2, 2].set_ylabel('True Digit')\n",
        "axes[2, 2].set_xticks(range(10))\n",
        "axes[2, 2].set_yticks(range(10))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Business Impact Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Business impact analysis for computer vision deployment\\nprint(\\\"\ud83d\udcb0 BUSINESS IMPACT ASSESSMENT\\\")\\nprint(\\\"=\\\"*50)\\n\\n# Simulate business metrics\\nn_daily_images = 50000  # Images processed per day\\nn_annual_images = n_daily_images * 365\\n\\n# Cost parameters\\ncost_per_misclassification = 2.50  # Cost of routing error\\ncost_per_manual_review = 0.15     # Human verification cost\\nrevenue_per_correct_classification = 0.05  # Processing fee\\n\\n# Calculate error rates and associated costs\\noriginal_error_rate = 1 - original_accuracy\\ncalibrated_error_rate = 1 - calibrated_accuracy\\n\\nprint(\\\"\ud83d\udcca OPERATIONAL METRICS:\\\")\\nprint(f\\\"   \u2022 Daily image volume: {n_daily_images:,}\\\")\\nprint(f\\\"   \u2022 Annual image volume: {n_annual_images:,}\\\")\\nprint(f\\\"   \u2022 Original error rate: {original_error_rate:.4f} ({original_error_rate*100:.2f}%)\\\")\\nprint(f\\\"   \u2022 Calibrated error rate: {calibrated_error_rate:.4f} ({calibrated_error_rate*100:.2f}%)\\\")\\n\\n# Annual cost comparison\\noriginal_annual_errors = n_annual_images * original_error_rate\\ncalibrated_annual_errors = n_annual_images * calibrated_error_rate\\nerror_reduction = original_annual_errors - calibrated_annual_errors\\n\\noriginal_error_cost = original_annual_errors * cost_per_misclassification\\ncalibrated_error_cost = calibrated_annual_errors * cost_per_misclassification\\nannual_cost_impact = original_error_cost - calibrated_error_cost\\n\\nprint(\\\"\\\\n\ud83d\udcb5 ANNUAL FINANCIAL IMPACT:\\\")\\nprint(f\\\"   \u2022 Original annual errors: {original_annual_errors:,.0f}\\\")\\nprint(f\\\"   \u2022 Calibrated annual errors: {calibrated_annual_errors:,.0f}\\\")\\nprint(f\\\"   \u2022 Error change: {error_reduction:,.0f} ({(error_reduction/original_annual_errors)*100:.1f}%)\\\")\\nif annual_cost_impact > 0:\\n    print(f\\\"   \u2022 Annual cost savings: ${annual_cost_impact:,.2f}\\\")\\nelse:\\n    print(f\\\"   \u2022 Annual cost increase: ${-annual_cost_impact:,.2f}\\\")\\n\\n# Confidence-based routing analysis\\nconfidence_threshold = 0.95\\nmax_calibrated_probs = np.max(y_proba_calibrated, axis=1)\\nmax_original_probs = np.max(y_proba, axis=1)\\n\\nhigh_conf_original = np.sum(max_original_probs >= confidence_threshold)\\nhigh_conf_calibrated = np.sum(max_calibrated_probs >= confidence_threshold)\\n\\nmanual_review_original = len(y_test) - high_conf_original\\nmanual_review_calibrated = len(y_test) - high_conf_calibrated\\n\\nprint(f\\\"\\\\n\ud83d\udd0d CONFIDENCE-BASED ROUTING (threshold = {confidence_threshold}):\\\")\\nprint(f\\\"   \u2022 Original: {high_conf_original}/{len(y_test)} auto-processed\\\")\\nprint(f\\\"   \u2022 Calibrated: {high_conf_calibrated}/{len(y_test)} auto-processed\\\")\\nprint(f\\\"   \u2022 Manual review change: {manual_review_calibrated - manual_review_original} samples\\\")\\n\\n# Scale to annual volume\\nannual_manual_original = (manual_review_original / len(y_test)) * n_annual_images\\nannual_manual_calibrated = (manual_review_calibrated / len(y_test)) * n_annual_images\\nannual_manual_change = annual_manual_original - annual_manual_calibrated\\n\\nmanual_cost_impact = annual_manual_change * cost_per_manual_review\\n\\nif manual_cost_impact > 0:\\n    print(f\\\"   \u2022 Annual manual review cost savings: ${manual_cost_impact:,.2f}\\\")\\nelse:\\n    print(f\\\"   \u2022 Annual manual review cost increase: ${-manual_cost_impact:,.2f}\\\")\\n\\n# Total business impact\\ntotal_annual_impact = annual_cost_impact + manual_cost_impact\\n\\nprint(\\\"\\\\n\ud83c\udfaf TOTAL BUSINESS IMPACT:\\\")\\nif total_annual_impact > 0:\\n    print(f\\\"   \u2022 Total annual savings: ${total_annual_impact:,.2f}\\\")\\n    roi_percentage = (total_annual_impact / (n_annual_images * revenue_per_correct_classification)) * 100\\n    print(f\\\"   \u2022 ROI on processing volume: {roi_percentage:.2f}%\\\")\\nelse:\\n    print(f\\\"   \u2022 Total annual cost increase: ${-total_annual_impact:,.2f}\\\")\\n    print(f\\\"   \u2022 Negative ROI: Calibration increases costs\\\")\\n\\n# Deployment recommendations based on actual performance\\nprint(\\\"\\\\n\ud83d\ude80 DEPLOYMENT RECOMMENDATIONS:\\\")\\n\\n# Check if rank preservation is poor\\nif np.mean(spearman_correlations) < 0.95:\\n    print(\\\"   \u274c NOT RECOMMENDED for production deployment\\\")\\n    print(\\\"   \ud83d\udcdd Rank preservation is severely compromised\\\")\\n    print(f\\\"   \ud83d\udcca Mean rank correlation: {np.mean(spearman_correlations):.3f} (target: >0.95)\\\")\\n    \\nelif calibrated_accuracy < original_accuracy - 0.02:\\n    print(\\\"   \u274c NOT RECOMMENDED for production deployment\\\")\\n    print(\\\"   \ud83d\udcdd Significant accuracy degradation detected\\\")\\n    print(f\\\"   \ud83d\udcca Accuracy drop: {calibrated_accuracy - original_accuracy:+.3f}\\\")\\n    \\nelif not result.converged:\\n    print(\\\"   \u26a0\ufe0f  CAUTION: Algorithm convergence issues\\\")\\n    print(\\\"   \ud83d\udcdd Consider alternative calibration methods\\\")\\n    print(f\\\"   \ud83d\udcca Failed to converge after {result.iterations} iterations\\\")\\n    \\nelse:\\n    print(\\\"   \u2705 Consider for production with monitoring\\\")\\n    print(\\\"   \ud83d\udcdd Performance appears acceptable but validate thoroughly\\\")\\n    recommendations = [\\n        \\\"Validate calibration on recent ZIP code data before deployment\\\",\\n        \\\"Monitor for concept drift in digit distribution patterns\\\",\\n        \\\"Implement A/B testing framework for comparison\\\",\\n        \\\"Plan for emergency fallback to original model if needed\\\"\\n    ]\\n    for i, rec in enumerate(recommendations, 1):\\n        print(f\\\"   {i}. {rec}\\\")\\n\\n# Risk assessment\\nprint(\\\"\\\\n\u26a0\ufe0f  IMPLEMENTATION CONSIDERATIONS:\\\")\\nconsiderations = [\\n    f\\\"Rank preservation quality: {np.mean(spearman_correlations):.3f}\\\",\\n    f\\\"Convergence status: {'Failed' if not result.converged else 'Success'}\\\",\\n    f\\\"Probability validity: {'Issues detected' if (has_negative or has_over_one or not row_sums_ok) else 'Valid'}\\\",\\n    \\\"Regulatory compliance requirements for postal accuracy\\\",\\n    \\\"Operational team training on new system behavior\\\"\\n]\\n\\nfor consideration in considerations:\\n    print(f\\\"   \u2022 {consideration}\\\")\\n\\n# Performance summary\\nprint(\\\"\\\\n\ud83d\udcc8 ALGORITHM PERFORMANCE SUMMARY:\\\")\\nprint(f\\\"   \u2022 Rank preservation: {np.mean(spearman_correlations):.6f} (1.0 = perfect)\\\")\\nprint(f\\\"   \u2022 Accuracy change: {calibrated_accuracy - original_accuracy:+.4f}\\\")\\nif has_negative or has_over_one or not row_sums_ok:\\n    print(f\\\"   \u2022 Probability issues: Required corrections\\\")\\nelse:\\n    print(f\\\"   \u2022 Probability quality: Valid output\\\")\\nprint(f\\\"   \u2022 Convergence: {'No' if not result.converged else 'Yes'} ({result.iterations} iterations)\\\")\\nprint(f\\\"   \u2022 Marginal accuracy: {np.max(np.abs(calibrated_marginals - target_marginals)):.2e} max error\\\")\\n\\n# Final recommendation\\nif (np.mean(spearman_correlations) >= 0.95 and \\n    calibrated_accuracy >= original_accuracy - 0.01 and \\n    result.converged and\\n    not (has_negative or has_over_one or not row_sums_ok)):\\n    print(\\\"\\\\n\u2705 FINAL ASSESSMENT: SUITABLE FOR PRODUCTION\\\")\\nelse:\\n    print(\\\"\\\\n\u274c FINAL ASSESSMENT: NOT READY FOR PRODUCTION\\\")\\n    print(\\\"   \ud83d\udcdd Consider alternative calibration methods or parameter tuning\\\")\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "This example demonstrated rank-preserving calibration for computer vision applications. The same principles apply broadly to:\n",
        "\n",
        "- **Medical imaging** with population-specific disease prevalence\n",
        "- **Autonomous systems** requiring calibrated uncertainty for safety\n",
        "- **Industrial automation** with domain-specific defect rates\n",
        "- **Content moderation** across platforms with different content distributions\n",
        "- **Retail applications** with store or region-specific product mix\n",
        "\n",
        "The key insight is that rank-preserving calibration maintains the model's core discriminative ability while adapting the probability estimates to match deployment conditions.\n",
        "\n",
        "For more examples in different domains, see the other notebooks:\n",
        "- Medical diagnosis with clinical population shifts\n",
        "- Text classification with domain adaptation\n",
        "- Financial risk assessment with portfolio-specific distributions\n",
        "- Survey reweighting for demographic correction"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
